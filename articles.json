[
  {
    "id": "2025-12-24-how-shopify-prepares-for-black-friday",
    "title": "How Shopify Prepares for Black Friday",
    "date": "2025-12-24",
    "preview": "Shopify 如何备战黑色星期五  注：本文是与 Shopify 工程团队合作撰写的。特别感谢 Shopify 工程团队与我们分享了他们为黑色星期五网络星期一（BFCM）所做的准备工作细节，并审阅了文章终稿。本文中分享的所有技术细节均归功于 Shopify 工程团队。  202...",
    "content": "Shopify 如何备战黑色星期五\n\n注：本文是与 Shopify 工程团队合作撰写的。特别感谢 Shopify 工程团队与我们分享了他们为黑色星期五网络星期一（BFCM）所做的准备工作细节，并审阅了文章终稿。本文中分享的所有技术细节均归功于 Shopify 工程团队。\n\n2024 年的黑色星期五网络星期一（BFCM）对 Shopify 来说是一场盛事。该平台处理了 57.3 PB 的数据，处理了 10.5 万亿次数据库查询，其边缘网络（edge network）的峰值请求达到每分钟 2.84 亿次。仅在应用服务器上，他们每分钟就处理了 8000 万次请求，并在黑色星期五期间每分钟推送 12 TB 的数据。\n\n有趣的是，这种流量水平现在已成为 Shopify 的基线。而 2025 年的 BFCM 规模更大，处理了 90 PB 的数据，1.75 万亿次数据库写入，峰值性能达到每分钟 4.89 亿次请求。正因如此，Shopify 彻底重建了其 BFCM 准备计划。\n\n准备工作涉及数千名工程师，历时九个月，进行了五次大规模的扩展性测试。\n\n在本文中，我们将探讨 Shopify 如何为这场商业界的“超级碗”做好准备。\n\n### 三轨并行框架\n\nShopify 的 BFCM 准备工作从三月份开始，在 Google Cloud 上采用了多区域（multi-region）策略。\n\n工程团队将工作分为三个并行轨道（tracks），它们同时运行并相互影响：\n\n**容量规划（Capacity Planning）**涉及利用历史数据和商家增长预测来建模流量模式。团队会提早将这些估算提交给其云服务提供商，以确保提供商有足够的物理基础设施可用。这项规划定义了 Shopify 需要多少计算能力以及这些能力在地理上应如何分布。\n\n**基础设施路线图（Infrastructure Roadmap）**是团队审查其技术栈（technology stack），评估需要进行哪些架构更改，并确定为达到目标容量所需的系统升级。此轨道有助于安排所有后续工作的顺序。重要的是，Shopify 从不将 BFCM 作为发布截止日期。所有的架构变更和迁移都会在关键窗口（critical window）前数月完成。\n\n**风险评估（Risk Assessments）**使用“可能出现什么问题”（What Could Go Wrong）的练习来记录故障场景。团队设定升级优先级，并为他们所谓的“Game Days”（演练日）提供输入。这些信息有助于他们提前测试和强化系统。\n\n这三个轨道之间持续相互影响。例如，风险评估结果可能会揭示团队未考虑到的容量缺口。基础设施变更可能会引入需要评估的新风险。换句话说，这是一个持续的反馈循环。\n\n### 演练日（Game Days）\n\n为了正确评估风险，Shopify 工程团队会进行“演练日”。这些是混沌工程（chaos engineering）演练，旨在有意识地模拟 BFCM 规模下的生产故障。\n\n团队在早春就开始举办演练日。这包括故意向系统注入故障，以测试它们在故障条件下的响应方式。可以将其视为软件的消防演习。\n\n在这些演练日期间，工程团队会特别关注他们所谓的“关键业务路径”（critical journeys）。这些是其平台中最关键的业务路径：结账、支付处理、订单创建和履约。如果在 BFCM 期间这些路径中断，商家会立即蒙受销售损失。\n\n关键业务路径的演练日会运行跨系统（cross-system）的灾难模拟。以下是团队通常测试的一些方面：\n\n*   团队测试搜索和页面端点（endpoints），同时随机化导航以模仿真实用户行为。他们注入网络故障和延迟，以观察服务无法快速通信时会发生什么。\n*   他们“清除缓存”（bust caches），以创建真实的负载模式，而不是当所有内容都被缓存时获得的人工快速响应。\n*   前端团队在这些演练中进行“找 Bug 大会”（bug bashes）。他们识别回归问题（regressions），测试关键用户流，并验证用户体验在峰值负载条件下是否能保持稳定。\n\n这些演练通过暴露运维手册（operational playbooks）和监控工具中的不足，帮助团队建立事故响应的“肌肉记忆”。\n\n最重要的是，Shopify 在 BFCM 之前就填补了这些空白，而不是在商家最需要平台时才发现问题。演练日的所有发现都会汇总到 Shopify 所称的“弹性矩阵”（Resiliency Matrix）中。这是一个集中式文档，用于跟踪整个平台的漏洞、事故响应程序和修复措施。\n\n弹性矩阵包含五个关键组件：\n\n1.  首先是**服务状态（service status）**，显示所有关键服务的当前运行状态。\n2.  其次是**故障场景（failure scenarios）**，记录系统可能如何崩溃以及会产生什么影响。\n3.  第三是**恢复程序（recovery procedures）**，列出预期的恢复时间目标（recovery time objectives）和详细的问题修复操作手册（runbooks）。\n4.  第四是**运维手册（operational playbooks）**，包含分步式事故响应指南。\n5.  第五是**值班覆盖（on-call coverage）**，显示团队排班和 PagerDuty 升级路径。\n\n该矩阵成为 BFCM 前系统强化的路线图。团队全年持续更新它，记录弹性改进。\n\n### 使用 Genghis 和 Toxiproxy 进行负载测试\n\n演练日测试的是独立组件，但 Shopify 还需要知道整个平台是否能处理 BFCM 的流量。这就是负载测试（load testing）的作用。\n\n工程团队构建了一个名为 Genghis 的工具，它运行模拟真实用户行为的脚本化工作流。它模拟浏览、将商品添加到购物车和完成结账流程。该工具逐渐增加流量，直到系统崩溃，这有助于团队找到其实际容量限制。\n\n测试在生产基础设施（production infrastructure）上同时从三个 Google Cloud 区域运行：us-central、us-east 和 europe-west4。这准确模拟了全球流量模式。Genghis 还在基线负载之上注入“闪购”（flash sale）爆发流量，以测试峰值容量场景。\n\nShopify 将 Genghis 与 Toxiproxy 结合使用，Toxiproxy 是他们构建的一个用于模拟网络条件的开源框架。Toxiproxy 注入网络故障和分区，阻止服务之间相互通信。例如，网络分区（network partition）是指系统的两个部分失去通信能力，即使它们都在运行。\n\n在测试期间，团队会实时监控仪表盘（dashboards），并准备在系统开始降级时中止测试。多个团队协调工作，及时发现并修复瓶颈。\n\n当负载测试揭示了限制时，团队有三个选择：\n\n*   **横向扩展（Horizontal scaling）**意味着增加更多的应用实例。\n*   **纵向扩展（Vertical scaling）**意味着为每个实例提供更多资源，例如 CPU 和内存。\n*   **优化（Optimizations）**意味着进行架构层面的更改以提高性能，范围从更好的数据库查询到跨消费层（consuming layers）乃至前端的性能调优。\n\n这些决策设定了最终的 BFCM 容量，并推动了 Shopify 整个技术栈（stack）的优化工作。关键的见解是，团队不能等到 BFCM 才发现容量限制。扩展基础设施和优化代码需要数月的准备。\n\n### 分析平台挑战\n\nBFCM 会考验 Shopify 的每一个系统，但 2025 年带来了一个独特的挑战。其部分基础设施从未经历过假日流量，这带来了一个问题：当没有历史数据可供建模时，如何为峰值负载做准备？\n\n2024 年，Shopify 的工程团队重建了整个分析平台。他们创建了新的 ETL 管道（pipelines）。ETL 代表 Extract（提取）、Transform（转换）、Load（加载），这是一个从各种来源拉取数据、处理数据并将其存储到有用位置的过程。他们还更换了持久化层（persistence layer），并用全新的 API 替换了其旧系统。\n\n这造成了一种不对称性。ETL 管道在 2024 年 BFCM 期间运行过，因此团队拥有一整季的生产数据，显示这些管道在假日负载下的性能。但他们的 API 层是在高峰期结束后才上线的。他们正在为 BFCM 准备那些从未经历过假日流量的 API。\n\n这非常重要，因为在 BFCM 期间，商家会痴迷地查看他们的分析数据。他们需要实时的销售数字、转化率、流量模式和热门产品数据。所有这些查询都会命中 API 层。如果这些 API 无法处理负载，商家在最关键的销售期将失去可见性。\n\nShopify 专门为分析基础设施运行了演练日。这些是受控实验，旨在揭示故障模式和瓶颈。团队模拟增加的流量负载，引入数据库延迟，并测试缓存故障，以系统地绘制系统在压力下的行为。\n\n结果显示了四个需要修复的关键问题：\n\n1.  首先，ETL 管道需要增加 Kafka 分区（partitions），以在流量高峰期间保持数据新鲜度。Apache Kafka 是一个处理实时数据流的分布式流处理平台。更多的分区意味着更多的并行处理，从而使 API 可以提供更新鲜的数据。\n2.  其次，API 层的内存使用需要优化。团队通过性能分析（profiling）发现了这一点，性能分析是指精确测量代码如何使用内存。每个 API 请求使用了过多的内存。在高负载下，这会导致内存不足错误（out-of-memory errors）、响应时间变慢或完全崩溃。\n3.  第三，连接超时（connection timeouts）需要调整以防止连接池耗尽（pool exhaustion）。连接池（connection pool）是一组可重用的数据库连接。创建新连接的开销很大，因此应用程序会重用它们。问题在于超时时间过长，这意味着连接会一直处于等待状态。在高负载下，可用连接会耗尽，新的请求将开始失败。Shopify 调整了超时时间以更快地释放连接。\n4.  第四，团队通过不同的负载均衡器（load balancer）方法来拆分 API 请求。最初，API 请求会全部排队到一个区域，这增加了延迟和负载。通过扩展辅助区域的集群并更新负载均衡策略，他们更好地分配了工作，防止 API 服务器过载。\n\n除了性能修复之外，团队还验证了警报机制（alerting）并记录了响应程序。他们的团队都经过培训，为在实际事件中处理故障做好了准备。\n\n### 规模测试\n\n演练日和负载测试是为单个组件做准备，而规模测试（scale testing）则不同。它验证整个平台在 BFCM 流量下协同工作的情况，揭示只有当所有系统同时满负荷运行时才会出现的问题。\n\n从四月到十月，Shopify 按照其预测的流量水平，特别是峰值 p90 流量假设，进行了五次大规模测试。在统计学中，p90 表示第 90 百分位，即 90% 的请求量都低于此流量水平。\n\n以下是这些规模测试的详细信息：\n\n*   前两次测试根据 2024 年的实际数据验证了基线性能。\n*   第三到第五次测试逐步提高到 2025 年的预测，目标是去年负载的 150%。\n*   到第四次测试时，Shopify 达到了每分钟 1.46 亿次请求和每分钟超过 8 万次结账。在当年的最后一次测试中，他们测试了 p99 场景，达到了每分钟 2 亿次请求。\n\n这些测试规模异常庞大，因此 Shopify 在夜间运行它们，并与 YouTube 协调，因为这些测试会影响共享的云基础设施。团队测试的是弹性（resilience），而不仅仅是原始负载容量。他们执行了区域故障转移（regional failovers），从美国和欧盟的核心区域疏散流量，以验证其灾难恢复程序（disaster recovery procedures）确实有效。\n\nShopify 运行了四种类型的测试：\n\n*   **架构扩展测试（Architecture scale-up tests）**验证了其基础设施能够处理计划的容量。\n*   **正常运行期间的负载测试（Load tests during normal operations）**建立了峰值负载下的基线性能。\n*   **带故障转移的负载测试（Load tests with failover）**验证了灾难恢复和跨区域故障转移能力。\n*   **演练日模拟（Game Day simulations）**通过混沌工程测试了跨系统的弹性。\n\n团队模拟了真实用户行为，例如店面浏览和结账、来自应用程序和集成的管理 API 流量、分析和报告负载以及后端 Webhook 处理。他们还测试了关键场景，如持续峰值负载、区域故障转移以及多个系统同时失败的级联故障（cascading failures）。\n\n每个测试周期都识别出在稳态负载下永远不会出现的问题，并且团队随着问题的出现而修复它们。一些关键问题如下：\n\n*   规模测试 1 和 2 揭示了在重负载下，核心操作会抛出错误，并且结账队列（checkout queues）会积压。\n*   规模测试 3 验证了关键迁移，并确认了基础设施变更后区域路由（regional routing）行为符合预期。\n*   规模测试 4 达到了限制，触发了计划外的故障转移，识别了测试流量路由中的优先问题，并发现了在重新平衡期间将区域重新上线时的延迟。\n*   规模测试 5 进行了全面预演，是唯一一次在北美工作时间运行的测试，以模拟真实的 BFCM 条件。所有其他测试都在夜间运行。\n\n在项目中期，Shopify 做出了一个重要的转变。他们将**认证结账流程（authenticated checkout flows）**添加到了测试场景中。模拟真实登录买家暴露了匿名浏览从未触及的限速代码路径。尽管认证流程只占流量的一小部分，但它们揭示了在实际事件中会导致问题的瓶颈。\n\n### BFCM 周末运营\n\nBFCM 的准备工作让 Shopify 做好了准备，但卓越的运营（operational excellence）确保他们在流量真正高峰时保持稳定。\n\n运营计划协调工程团队、事故响应和实时系统调优。以下是该计划的关键组成部分：\n\n*   BFCM 周末的计划包括实时监控（real-time monitoring），提供所有区域的仪表盘可见性（dashboard visibility）和自动化警报（automated alerts）。\n*   对于事故响应，事故经理（Incident Manager）值班团队提供 24/7 全天候覆盖，并有明确的升级路径。\n*   商家沟通确保商店获得状态更新和任何问题的通知。\n*   实时优化（Live optimization）允许根据实时流量模式进行系统调优。\n*   BFCM 结束后，事后分析（post-mortem）过程将监控数据与实际商家结果关联起来，以了解哪些方法有效以及哪些地方需要改进。\n\n理念很简单：准备让你做好准备，但卓越的运营让你保持稳定。\n\n### 结论\n\nShopify 2025 年的 BFCM 准备计划展示了大规模系统准备工作是怎样的。数千名工程师工作了九个月，进行了五次大规模的扩展性测试，将其基础设施推向了预期负载的 150%。他们执行了区域故障转移，运行了混沌工程演练，记录了系统漏洞，并在商家需要之前，用更新的操作手册强化了系统。\n\n这与典型的发布前准备不同之处在于其系统化的方法。大多数公司进行一两次负载测试，修复关键 bug，然后寄希望于最佳结果。Shopify 则花费了九个月的时间持续测试，发现系统崩溃点，修复问题，并验证修复措施确实有效。\n\n此外，Shopify 构建的工具并非临时的 BFCM 支撑结构。弹性矩阵（Resiliency Matrix）、关键业务路径演练日（Critical Journey Game Days）和实时自适应预测（real-time adaptive forecasting）都成为了永久性的基础设施改进。它们使 Shopify 每天都更具弹性，而不仅仅是在高峰期。\n\n为了提供 BFCM 的可视化效果，Shopify 还推出了一款有趣的弹珠游戏，以展示 Shopify Live Globe。该游戏本身在浏览器中以 120fps 运行，拥有完整的 3D 环境、物理引擎和 VR 支持。在幕后，这款游戏是一个使用“react-three-fiber”构建的 three.js 应用程序。每一笔商家销售都会在几秒钟后显示在这个地球仪上。\n\n参考资料：\n*   How we prepare Shopify for BFCM (Shopify 如何为 BFCM 做准备)\n*   Extract, Transform, Load (提取、转换、加载 - ETL)\n*   Toxiproxy\n*   Shopify Live Globe\n*   Details about the Shopify Live Globe Pinball Game (关于 Shopify Live Globe 弹珠游戏的详细信息)\n\n---\n\n### 要点总结\n\n*   Shopify 采用系统化的“三轨并行框架”准备 BFCM：容量规划、基础设施路线图和风险评估，三者持续相互反馈。\n*   通过“演练日”（Game Days）进行混沌工程，模拟生产故障，特别关注结账、支付等“关键业务路径”。\n*   建立了“弹性矩阵”（Resiliency Matrix）作为集中的文档，记录服务状态、故障场景、恢复程序、运维手册和值班安排，指导系统强化。\n*   使用自研工具 Genghis 进行全平台负载测试，模拟真实用户行为和闪购流量，并通过 Toxiproxy 模拟网络故障。\n*   负载测试中发现容量限制时，采取横向扩展、纵向扩展或架构优化等策略。\n*   对新构建的分析平台，通过专门的演练日识别并解决了 Kafka 分区不足、API 内存使用过高、连接超时及负载均衡不合理等问题。\n*   从四月到十月进行了五次大规模测试，覆盖 p90 和 p99 峰值流量，并执行了区域故障转移，验证灾难恢复能力。\n*   测试场景中增加了认证结账流程，揭示了匿名浏览无法触及的限速瓶颈。\n*   BFCM 期间的运营侧重于实时监控、自动化警报、24/7 事故响应、商家沟通和实时系统调优，并通过事后分析持续改进。\n*   所建立的弹性矩阵、演练日和实时自适应预测等机制是永久性的基础设施改进，而非临时措施，持续提升系统弹性。\n\n---\n\n### 你可以从这篇文章学到什么\n\n对于一个拥有几年经验的后端/系统设计工程师来说，这篇文章提供了一个大型电商平台如何系统性地应对流量洪峰的宝贵案例。你可以从中学习到以下几点，并应用于实际工作中：\n\n1.  **分阶段、系统化的准备方法**：Shopify 的“三轨并行框架”——容量规划、基础设施路线图和风险评估——展示了如何在早期并行开展多项工作，并形成反馈闭环。这对于任何大型项目的准备都非常有借鉴意义，它强调了规划、实施和风险管理需同步进行，而非线性推进。\n\n2.  **混沌工程与“关键业务路径”**：文章详细介绍了“演练日”如何通过故意注入故障来测试系统弹性，并强调了聚焦“关键业务路径”（如结账、支付）的重要性。在你的系统设计中，识别并持续测试最核心的业务流程，能够确保在极端情况下业务不受致命影响。你可以将混沌工程的理念引入自己的测试策略，而不仅仅是简单的单元测试和集成测试。\n\n3.  **可操作的“弹性矩阵”**：弹性矩阵作为集中式文档，涵盖了服务状态、故障场景、恢复程序、运维手册和值班覆盖。这提供了一个构建自身灾难恢复和应急响应体系的清晰蓝图。一个详尽且持续更新的矩阵，能够显著提高团队在突发事件中的响应效率和准确性。\n\n4.  **精细化的负载与规模测试**：Genghis 和 Toxiproxy 这类工具的运用，以及多区域、多场景（正常负载、闪购爆发、带故障转移）的测试方法，展示了如何真实地模拟生产环境。尤其是对新系统（如分析平台 API）没有历史数据的情况，通过专门的演练日来发现和解决问题，提供了应对“数据盲点”的有效策略。在你的项目中，即使没有自研工具，也可以利用现有的负载测试工具和混沌工程框架，设计更贴近真实世界的测试方案。\n\n5.  **深入的问题分析与解决**：文章中分析平台遇到的四个问题及其解决方案（Kafka 分区调整、API 内存优化、连接超时调优、负载均衡策略调整），都是后端工程师在日常工作中可能遇到的典型性能瓶颈。了解这些具体的优化手段，能帮助你在面对类似问题时，有更清晰的排查思路和解决方案。\n\n6.  **早期发现和持续改进**：Shopify 强调不能等到峰值才发现问题，而是要提前数月持续测试和修复。这种“持续测试-发现-修复-验证”的循环，以及将临时应对措施转化为永久性基础设施改进（如弹性矩阵成为日常工具）的理念，是提升系统长期稳定性和可维护性的关键。\n\n总之，这篇文章不仅仅是关于如何应对黑色星期五，更是关于如何构建一个高度弹性、可伸缩且运维卓越的大规模分布式系统。它鼓励工程师从更宏观的视角去思考系统准备、风险管理和持续改进。",
    "url": "https://blog.bytebytego.com/p/how-shopify-prepares-for-black-friday"
  }
]