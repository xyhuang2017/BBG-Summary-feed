[
  {
    "id": "2025-12-23-multimodal-llms-basics-how-llms-process-text-images-audio-videos",
    "title": "Multimodal LLMs Basics: How LLMs Process Text, Images, Audio & Videos",
    "date": "2025-12-23",
    "preview": "# 多模态LLM基础：LLM如何处理文本、图像、音频和视频  很长一段时间以来，AI系统都是专注于单一感知的专家。例如： *   计算机视觉模型可以识别照片中的物体，但无法描述它们看到的东西。 *   自然语言处理 (Natural Language Processing, NL...",
    "content": "# 多模态LLM基础：LLM如何处理文本、图像、音频和视频\n\n很长一段时间以来，AI系统都是专注于单一感知的专家。例如：\n*   计算机视觉模型可以识别照片中的物体，但无法描述它们看到的东西。\n*   自然语言处理 (Natural Language Processing, NLP) 系统可以写出优美的散文，但对图像却“视而不见”。\n*   音频处理模型可以转录语音，但缺乏视觉上下文。\n\n这种碎片化与人类体验世界的方式根本不同。人类认知本质上是多模态的。我们不只是阅读文本或只看图片。我们同时观察面部表情，同时听音调。我们将狗的视觉形状与吠叫声和书面词语“dog”联系起来。\n\n为了创建真正在现实世界中运行的AI，这些分离的感官通道需要融合。\n\n多模态大型语言模型 (Multimodal Large Language Models) 代表了这种融合。例如，GPT-4o 可以在232毫秒内响应语音输入，与人类对话速度相匹配。谷歌的 Gemini 可以在单个提示中处理一小时的视频。\n\n这些能力源于一个统一的神经网络，它可以同时看、听和读。\n\n但是，一个AI系统如何理解如此根本不同的数据类型呢？在本文中，我们将尝试回答这个问题。\n\n## 共享的数学语言\n\n多模态LLMs的核心突破非常简单。每种输入类型，无论是文本、图像还是音频，都被转换成相同类型的数学表示，称为嵌入向量 (embedding vectors)。正如人脑将光子、声波和书面符号转换为统一的神经信号一样，多模态LLMs将不同数据类型转换为占据相同数学空间的向量。\n\n让我们考虑一个具体的例子。一张狗的照片，口语词“dog”，以及书面文本“dog”都被转换成高维数学空间中的点。这些点会聚集在一起，彼此靠近，因为它们代表相同的概念。\n\n这种统一的表示方式实现了研究人员所说的跨模态推理 (cross-modal reasoning)。模型可以理解吠叫声、金毛寻回犬的照片以及“这只狗很开心”这句话都与同一个底层概念相关。模型不需要为每种模态配备单独的系统。相反，它通过一个单一的架构处理所有内容，将视觉图像块 (visual patches) 和音频片段 (audio segments) 像文本 token 一样对待。\n\n## 三部分架构：多模态LLM的构建块\n\n（此处原文章包含一张示意图，展示多模态LLM的高级工作原理）\n\n现代多模态LLMs由三个协同工作的基本组件组成，以处理多样化输入。\n\n### 特定模态编码器 (Modality-Specific Encoders)\n\n第一个组件负责将原始感官数据转换为初始数学表示。\n*   视觉 Transformer (Vision Transformers) 通过将图像视为句子来处理它们，将照片分成小图像块 (patches)，并像处理单词一样处理每个图像块。\n*   音频编码器 (Audio encoders) 将声波转换为频谱图 (spectrograms)，这是一种类似视觉的表示，显示频率如何随时间变化。\n\n这些编码器通常在海量数据集上进行预训练，以便在各自的任务中变得非常熟练。\n\n### 投影层 (Projection Layers)：翻译器\n\n第二个组件充当桥梁。尽管两种编码器都生成向量，但这些向量存在于不同的数学空间中。换句话说，视觉编码器对“cat”的表示与语言模型对单词“cat”的表示存在于不同的几何区域中。\n\n投影层 (Projection layers) 将这些不同的表示对齐到语言模型操作的共享空间中。通常，这些投影器出奇地简单，有时只是一个线性变换或一个小的两层神经网络。尽管它们很简单，但它们对于使模型理解视觉和听觉概念至关重要。\n\n### 语言模型主干 (Language Model Backbone)\n\n第三个组件是核心 LLM，例如 GPT 或 LLaMA。\n\n这是进行实际推理并生成响应的“大脑”。它接收所有输入作为 token 序列，无论这些 token 是来自文本、图像块还是音频片段。\n\n语言模型对其一视同仁，通过与纯文本模型相同的 Transformer 架构处理所有内容。这种统一处理使得模型能够像处理纯文本一样自然地进行跨模态推理。\n\n（此处原文章包含一张示意图，展示 Transformer 架构）\n\n## 图像如何变成LLM能理解的东西\n\n使现代多模态视觉成为可能的一项突破来自于2020年一篇标题令人难忘的论文：“一张图像值16x16个单词 (An Image is Worth 16x16 Words)。” 这篇论文提出了将图像像句子一样处理的想法，将小图像块 (patches) 视为 token。\n\n这个过程通过几个步骤进行：\n*   首先，图像被分成固定大小的图像块网格，通常每个图像块为16x16像素。\n*   一个标准的224x224像素图像大约会变成196个不同的图像块，每个图像块代表一个小的方形区域。\n*   每个图像块从2D网格展平为1D数字向量，表示像素强度。\n*   添加位置编码 (positional embeddings)，以便模型知道每个图像块在原始图像中的来源。\n*   这些图像块嵌入通过 Transformer 层，其中注意力机制 (attention mechanisms) 允许图像块相互学习。\n\n注意力机制是理解产生的地方。显示狗耳朵的图像块学习它与显示狗脸和身体的附近图像块相关联。描绘海滩场景的图像块学习相互关联，以表示沙滩和水的更广泛上下文。通过最后一层，这些视觉 token 携带丰富的上下文信息。模型不仅仅看到“棕色像素”，而是理解“金毛寻回犬坐在海滩上”。\n\n### OpenAI CLIP\n\n第二个关键创新是 OpenAI 开发的 CLIP。CLIP 通过改变基本目标，彻底改变了视觉编码器的训练方式。CLIP 不是在带标签的图像类别上进行训练，而是在来自互联网的4亿对图像及其文本描述上进行训练。\n\nCLIP 使用对比学习 (contrastive learning) 方法。给定一批图像-文本对，它计算所有图像和所有文本描述的嵌入。目标是最大化正确图像-文本对嵌入之间的相似性，同时最小化不正确配对之间的相似性。一张狗的图像应该产生一个与“公园里的一只狗”的标题接近的向量，但与“一盘意大利面”的标题相距甚远。\n\n## 音频如何变得可理解\n\n音频给语言模型带来了独特的挑战。\n与自然地分成离散单词的文本或可以分成空间图像块的图像不同，声音是连续的、时态的。例如，一个以16,000赫兹采样、30秒的音频片段包含480,000个独立数据点。将这个巨大的数字流直接输入 Transformer 是计算上不可能且低效的。解决方案需要将音频转换为更易处理的表示。\n\n关键创新是将音频转换为频谱图 (spectrograms)，这本质上是声音的图像。这个过程涉及几个数学变换：\n*   长音频信号被切分成微小的重叠窗口，通常每个25毫秒。\n*   快速傅里叶变换 (Fast Fourier Transform) 提取每个窗口中存在的频率。\n*   这些频率被映射到 Mel 尺度 (mel scale)，该尺度通过赋予较低频率更高的分辨率来匹配人类听觉敏感度。\n*   结果是一个2D热图，其中时间沿着一个轴运行，频率沿着另一个轴，颜色强度表示音量。\n\n这种 Mel 频谱图 (mel-spectrogram) 对AI模型来说就像一张图像。对于一个30秒的片段，这可能会产生一个80x3,000的网格，这本质上是声学模式的视觉表示，可以像处理照片一样进行处理。\n一旦音频被转换为频谱图，模型就可以应用用于视觉的相同技术。音频频谱 Transformer (Audio Spectrogram Transformer) 将频谱图分成图像块 (patches)，就像图像被分割一样。例如，像 Whisper 这样在680,000小时多语言音频上训练的模型，在这方面表现出色。\n\n## 训练过程\n\n训练过程分为不同阶段：\n\n### 阶段1：特征对齐 (Feature Alignment)\n\n多模态LLM的训练通常分两个不同阶段进行。\n第一阶段纯粹侧重于对齐 (alignment)，教导模型相同概念的视觉和文本表示应该相似。在此阶段，预训练的视觉编码器 (vision encoder) 和预训练的语言模型 (language model) 保持冻结。只有投影层 (projection layer) 的权重通过训练进行更新。\n\n### 阶段2：视觉指令微调 (Visual Instruction Tuning)\n\n仅靠对齐不足以进行实际使用。模型可能能够描述图像中的内容，但在复杂任务上会失败，例如“这个人为什么看起来很悲伤？”或“比较这两个图表”。\n视觉指令微调 (Visual instruction tuning) 通过训练模型遵循复杂的​​多模态指令来解决这个问题。\n在此阶段，投影层继续训练，语言模型也进行更新，通常使用参数高效方法 (parameter-efficient methods)。训练数据转向以对话形式格式化的指令-响应数据集。\n这里的一个重要创新是使用 GPT-4 生成合成训练数据 (synthetic training data)。研究人员向 GPT-4 提供图像的文本描述，并提示它创建关于这些图像的逼真对话。在这种合成但高质量的数据上进行训练，有效地将 GPT-4 的推理能力蒸馏 (distills) 到多模态模型中，教导它进行细致入微的视觉对话，而不仅仅是描述它所看到的内容。\n\n## 结论\n\n多模态LLMs通过一个统一的原则实现了其卓越的能力。通过将所有输入转换为占据共享数学空间的嵌入向量序列，单个 Transformer 架构可以像处理纯语言一样流畅地进行跨模态推理。\n\n推动这种能力的架构创新代表了真正的进步：视觉 Transformer 将图像视为视觉句子，对比学习 (contrastive learning) 无需显式标签即可对齐模态，以及交叉注意力 (cross-attention) 实现了不同数据类型之间的选择性信息检索。\n\n未来指向“任意到任意” (any-to-any) 模型，它们可以理解和生成所有模态。换句话说，一个模型可以在单个响应中输出文本、生成图像并合成语音。\n\n---\n\n## 要点总结\n\n*   多模态LLM (Multimodal LLM) 旨在融合不同感官数据（文本、图像、音频、视频），以实现更接近人类的认知。\n*   核心思想是将所有模态数据统一转换为高维嵌入向量 (embedding vectors)，使其在共享的数学空间中表示。\n*   这种统一表示使模型能够进行跨模态推理 (cross-modal reasoning)，理解不同模态中相同概念的关联。\n*   多模态LLM通常由三部分组成：特定模态编码器 (Modality-Specific Encoders)、投影层 (Projection Layers) 和语言模型主干 (Language Model Backbone)。\n*   特定模态编码器负责将原始数据（如图像、音频）转换为初始向量表示，例如 Vision Transformers 处理图像补丁，音频编码器生成频谱图。\n*   投影层是关键的桥梁，它将不同编码器生成的向量对齐到语言模型能理解的共享数学空间。\n*   语言模型主干是核心推理引擎，它将所有输入（无论来自何种模态）视为统一的 token 序列进行处理。\n*   图像通过分割成图像块 (patches) 并添加位置编码，然后通过 Transformer 架构进行处理，实现从像素到语义的理解。\n*   音频通过转换为 Mel 频谱图 (mel-spectrograms)，然后像图像一样被分割成补丁，通过 Audio Spectrogram Transformer 等模型进行处理。\n*   训练过程通常包括两个阶段：特征对齐 (Feature Alignment)（冻结编码器和LLM，仅训练投影层）和视觉指令微调 (Visual Instruction Tuning)（更新投影层和LLM，使用指令-响应数据集，有时用 GPT-4 生成合成数据）。\n\n## 你可以从这篇文章学到什么\n\n对于一个拥有几年经验的后端/系统设计工程师来说，这篇文章不仅介绍了多模态LLM的内部机制，更提供了许多在系统设计中可以借鉴的理念和趋势：\n\n1.  **数据统一表示和抽象能力的重要性**：多模态LLM的核心是将所有不同类型的数据（文本、图像、音频）转换为统一的嵌入向量。这启发我们在设计大型系统时，应思考如何将多样化的数据来源和格式抽象为统一的中间表示。这样做可以极大地简化下游服务的处理逻辑，提高系统的可扩展性和灵活性，例如设计通用的数据接入层或数据格式转换服务。\n2.  **模块化架构与专业化组件设计**：文章描述了“编码器-投影层-语言模型主干”的三层架构。这是一种典型的模块化设计思想，每个组件专注于特定任务。在设计微服务或分布式系统时，我们可以借鉴这种专业分工：将数据预处理、数据转换、核心业务逻辑（或AI推理）等职责清晰地划分给不同的服务或模块，实现高内聚、低耦合。\n3.  **跨系统/跨模态数据对齐与集成**：投影层作为不同模态编码器输出的“翻译器”，解决了不同数据表示之间的语义对齐问题。在实际的系统集成中，这对应着如何有效地桥接不同系统（如遗留系统与新服务、不同数据源）的数据和API。我们需要设计明确的数据转换层、API网关或数据协调服务，确保数据在不同系统间流转时语义一致且格式兼容。\n4.  **数据预处理和特征工程的通用模式**：无论是将图像分割成小块，还是将音频转换为频谱图，这些都是将原始、复杂数据转化为AI模型易于处理的“特征”的过程。对于系统工程师而言，理解这些数据处理模式有助于设计高效、可扩展的数据管道，用于清洗、转换和提取数据特征，从而为机器学习模型提供高质量的输入。\n5.  **对未来AI发展趋势的洞察**：文章预示了“任意到任意” (any-to-any) AI模型的未来。这意味着未来的系统可能需要处理和生成多种模态的数据。作为系统设计者，应提前规划更灵活、可扩展的架构，以适应这种多模态的输入输出需求。例如，数据存储方案可能需要支持不同类型的数据，消息队列可能需要处理多模态消息，并且需要考虑如何将不同模态的生成能力集成到用户体验中。\n6.  **训练/部署策略的启示**：分阶段训练（特征对齐和指令微调）的思路，也为复杂系统或AI模型的迭代开发提供了借鉴。可以先构建一个具有基础能力的系统核心（对齐阶段），然后通过有针对性的“微调”（如A/B测试、用户反馈驱动的迭代）来提升其在特定场景下的性能和用户体验。这有助于控制开发风险，并逐步优化系统。",
    "url": "https://blog.bytebytego.com/p/multimodal-llms-basics-how-llms-process"
  },
  {
    "id": "2025-12-22-ep194-evolution-of-http",
    "title": "EP194: Evolution of HTTP",
    "date": "2025-12-22",
    "preview": "EP194: HTTP 的演进 ByteByteGo 2025年12月20日 177 5 4 分享 ✂️ 使用 QA Wolf 将您的 QA 周期缩短至几分钟（赞助） 如果缓慢的 QA 流程阻碍了您或您的软件工程团队，并因此导致发布速度变慢，那么您需要了解一下 QA Wolf。 ...",
    "content": "EP194: HTTP 的演进\nByteByteGo\n2025年12月20日\n177\n5\n4\n分享\n✂️ 使用 QA Wolf 将您的 QA 周期缩短至几分钟（赞助）\n如果缓慢的 QA 流程阻碍了您或您的软件工程团队，并因此导致发布速度变慢，那么您需要了解一下 QA Wolf。\nQA Wolf 的 AI 原生服务\n支持网页和移动应用\n，可在\n数周内实现 80% 的自动化测试覆盖率\n，并通过将 QA 周期缩短至几分钟，帮助团队\n交付速度提高 5 倍\n。\nQA Wolf\n将测试从您的任务清单中移除。他们可以为您提供：\n移动和网页应用的无限并行测试运行\n24 小时维护和按需测试创建\n人工验证的 Bug 报告直接发送给您的团队\n保证零故障（Zero flakes）\n益处何在？告别手动 E2E 测试，告别缓慢的 QA 周期，告别 Bug 进入生产环境。\n借助 QA Wolf，\nDrata 拥有 80 多名工程师的团队\n实现了 4 倍的测试用例增长和\n86% 更快的 QA 周期\n。\n安排演示以了解更多信息\n本周的系统设计回顾：\nHTTP 的演进\n每个工程师都应该了解的系统性能指标\nNginx 为何如此受欢迎？\n每个工程师都应该了解的网络调试命令\n集线器 (Hub)、交换机 (Switch) 和路由器 (Router) 解释\n赞助我们\nHTTP 的演进\n超文本传输协议 (HTTP) 经过多年演进，以满足现代应用的需求，从简单的文本传输到高性能、实时体验。\n以下是 HTTP 的发展历程：\nHTTP/0.9：为使用单个 GET 请求获取简单 HTML 文档而构建。\nHTTP/1.0：添加了 headers（请求头）和 status codes（状态码）以支持更丰富的交互，但每个请求仍然需要一个新的连接。\nHTTP/1.1：引入了 persistent connections（持久连接）和更多方法，使日常网页浏览更快、更高效。\nHTTP/2：通过 multiplexing（多路复用）解决了性能瓶颈，允许多个请求共享一个连接。\nHTTP/3 (QUIC)：通过 QUIC 转向 UDP，以降低 latency（延迟）并提高 reliability（可靠性），尤其适用于移动和实时应用程序。\n轮到你了：您的项目中是否已经在使用 HTTP/3 了？\n2026 OKR：利用 AI 将 MTTR 提速 70% 并优化成本（赞助）\n来自 Claude 的代码即将上线生产环境，但这不一定痛苦。\nCoinbase、Toast、Gametime、MSCI 和 Zscaler 的工程团队使用 Resolve AI 来解决 incident（事件），优化成本，并利用跨代码、基础设施 (infra) 和遥测 (telemetry) 工作的 AI，结合生产上下文进行构建。\n结果是 MTTR（平均恢复时间）加快 70%，每个事件所需工程师数量减少 30%，并节省了数千小时的工程时间。想象一下，2026 年您可以利用这些时间交付什么？\n了解更多关于 AI for Prod、workflow-autonomous multi-agent systems（工作流自主多智能体系统），以及如何削减 orchestration tax（编排成本）、改进调查，并将工程师的时间从繁琐工作中转移到更重要的工作中。\n下载免费的《AI for Prod》电子书\n每个工程师都应该了解的系统性能指标\n您的 API 很慢。但到底有多慢？您需要数据。真正的指标可以告诉您具体哪里出了问题以及如何修复。\n以下是每个工程师在分析系统性能时都应该了解的四个核心指标：\n每秒查询数 (QPS)：您的系统每秒处理多少个传入请求。您的服务器在一秒内收到 1,000 个请求？那就是 1,000 QPS。这听起来很简单，直到您意识到大多数系统无法长时间维持其峰值 QPS 而不出问题。\n每秒事务数 (TPS)：您的系统每秒处理多少个已完成的事务。一个 transaction（事务）包括完整的往返过程，即请求发出、命中数据库并返回响应。\nTPS 告诉您实际完成的工作量，而不仅仅是收到的请求。这是您的业务所在意的。\n并发数 (Concurrency)：您的系统在任何给定时刻处理的 simultaneous active requests（同时活跃请求）数量。您可能每秒有 100 个请求，但如果每个请求需要 5 秒才能完成，那么您实际上同时处理着 500 个并发请求。\n高并发意味着您需要更多资源、更好的 connection pooling（连接池）以及更智能的 thread management（线程管理）。\n响应时间 (RT)：从请求开始到收到响应所经过的时间。在 client（客户端）和 server（服务器）级别都进行测量。\n一个简单的关系将它们联系在一起：QPS = Concurrency ÷ Average Response Time（平均响应时间）\n更高的 Concurrency 或更低的 Response Time = 更高的 throughput（吞吐量）。\n轮到你了：当您分析性能时，您首先查看哪个指标：QPS、TPS 还是 Response Time？\nNginx 为何如此受欢迎？\nApache 在 web 服务器领域主导了 20 年，然后 Nginx 出现并改变了一切。现在，Nginx 为包括 Netflix、Airbnb、Dropbox 和 WordPress.com 在内的一些互联网上最大的网站提供支持。这并非因为它更新潮，而是因为它解决了 Apache 无法高效处理的问题。\nNginx 如此受欢迎的原因有：\n高性能 Web 服务器\n反向代理与负载均衡器\n缓存层\nSSL 终结 (Offloading)\n轮到你了：目前您主要将 Nginx 用于什么：web 服务器、反向代理还是负载均衡器？\n每个工程师都应该了解的网络调试命令\n当有人说“这是网络问题”时，这些命令可以帮助您快速找到问题所在。\nping：检查目标是否响应，并报告往返时间以确认基本可达性。\ntraceroute / tracert：显示路径上的每个 hop（跳），以便您可以看到数据包在哪里变慢或停止。\nmtr / pathping：持续测量每个 hop 的 latency（延迟）和 loss（丢包），以发现间歇性问题。\nip addr, ip link / ipconfig /all：打印本地 IP、MAC 地址和 interface（接口）状态，以便您验证机器的网络身份。\nip route：显示 routing table（路由表），以确认系统将使用哪个 gateway（网关）和 next hop（下一跳）。\nip neigh：显示 IP 到 MAC 的条目，以检测 LAN 上的重复或过时的 ARP 记录。\nss -tulpn：列出监听中的 sockets（套接字）和 PIDs（进程 ID），以便您确认服务是否确实绑定到预期端口。\ndig：解析 DNS 记录，以验证客户端将连接到的确切 IP 地址。\ncurl -I：仅获取 HTTP(S) headers（请求头），以检查 status codes（状态码）、redirects（重定向）和 cache settings（缓存设置）。\ntcpdump / tshark：捕获数据包，以便您检查实际流量并验证发送和接收的内容。\niperf3：测量两个主机之间的端到端 throughput（吞吐量），以区分带宽限制和应用程序问题。\nssh：在远程机器上打开 secure shell（安全外壳），直接运行检查和应用修复。\nsftp：安全传输文件，以便您在事件期间拉取日志或推送 artifact（工件）。\nnmap：扫描开放端口并探测版本，以确认哪些服务已暴露并正在响应。\n轮到你了：当您调试网络问题时，您首选的命令是什么？\n集线器 (Hub)、交换机 (Switch) 和路由器 (Router) 解释\n每个家庭和办公室网络都依赖于这三种设备：hub（集线器）、switch（交换机）和 router（路由器），但它们的角色常常被混淆。\nhub 在 Layer 1（物理层）工作。它是三者中最简单的，不理解地址或数据类型。当数据包到达时，它只是将其广播到所有连接的设备，创建一个大的 collision domain（冲突域）。这意味着所有设备都争用相同的带宽，使得 hub 在现代网络中效率低下。\nswitch 在 Layer 2（数据链路层）工作。它学习 MAC addresses（MAC 地址）并将 frames（帧）仅转发到正确的目的地设备。switch 上的每个端口都充当其自己的 collision domain（冲突域），提高了 LAN（局域网）内部通信的效率和速度。\nrouter 在 Layer 3（网络层）工作。它根据 IP addresses（IP 地址）路由数据包，并将不同的网络连接在一起，例如您的家庭网络连接到互联网。每个 router interface（路由器接口）都形成一个独立的 broadcast domain（广播域），使本地和外部流量隔离。\n了解这三个层如何协同工作是每个现代网络的基础，从您的家庭 Wi-Fi 到全球互联网骨干网。\n轮到你了：您通常如何判断网络问题是由 router 还是 switch 引起的？\n赞助我们\n让您的产品呈现在超过 1,000,000 名技术专业人士面前。\n我们的新闻通讯将您的产品和服务直接呈现在重要的受众面前——数十万的工程领导者和高级工程师——他们对重要的技术决策和重大采购具有影响力。\n席位很快被占满——立即预订\n广告位通常提前约 4 周售罄。为确保您的广告能够触达这批有影响力的受众，请立即发送电子邮件至 sponsorship@bytebytego.com 预订。\n177\n5\n4\n分享\n\n---\n\n## 要点总结\n\n*   **HTTP 的演进：** HTTP 从最初的简单文本传输（HTTP/0.9）发展到支持请求头、状态码和持久连接（HTTP/1.0, HTTP/1.1），并通过多路复用（HTTP/2）和基于 UDP 的 QUIC 协议（HTTP/3）持续优化性能、降低延迟并提高可靠性。\n*   **系统性能核心指标：** QPS（每秒查询数）、TPS（每秒事务数）、Concurrency（并发数）和 Response Time（响应时间）是衡量系统性能的关键指标，它们之间存在相互关系，例如 QPS = Concurrency ÷ Average Response Time。\n*   **Nginx 的多功能性：** Nginx 不仅是高性能的 Web 服务器，更广泛应用于反向代理、负载均衡器、缓存层和 SSL 终结（卸载），解决了传统服务器在高并发场景下的效率问题。\n*   **网络调试命令：** 文章提供了包括 ping、traceroute、mtr、ip addr、ss、dig、curl -I、tcpdump、iperf3、ssh、sftp 和 nmap 在内的多种网络调试命令，这些是工程师快速定位和解决网络问题的必备工具。\n*   **网络设备分层理解：** Hub（集线器）工作在物理层（Layer 1），进行广播；Switch（交换机）工作在数据链路层（Layer 2），通过 MAC 地址转发并创建独立的冲突域；Router（路由器）工作在网络层（Layer 3），基于 IP 地址路由并连接不同网络，创建独立的广播域。理解这些设备及其工作层次是构建和维护现代网络的基础。\n\n## 你可以从这篇文章学到什么\n\n对于一个有几年经验的后端/系统设计工程师来说，这篇文章提供了一个对核心系统设计和网络概念的快速而全面的回顾。虽然这些概念可能并非全新，但它们构成了构建和维护高可用、高性能系统的基石。以下是你可以从中学到并应用到实际项目中的几点：\n\n1.  **深化对协议选择的理解：** 了解 HTTP 的演进，特别是 HTTP/2 和 HTTP/3 (QUIC) 如何通过多路复用和基于 UDP 降低延迟来解决性能瓶颈。在设计新的微服务或客户端-服务器通信时，你可以根据应用场景（例如，移动应用或实时通信）考虑采用 HTTP/3，以实现更优的性能和用户体验。这不仅仅是“用更新的协议”那么简单，而是理解其底层优势，从而做出更明智的技术选型。\n\n2.  **精确衡量和优化系统性能：** 这篇文章清晰地定义了 QPS、TPS、Concurrency 和 Response Time 这四个关键性能指标及其相互关系（QPS = Concurrency ÷ Average Response Time）。作为一个工程师，你不仅要会看这些指标，更要理解它们背后的含义。例如，当系统响应时间变慢时，你可以通过分析 Concurrency 和 QPS 的变化来判断是请求量激增、处理能力瓶颈还是单个请求耗时过长导致的问题。在设计负载测试、监控系统或进行性能调优时，这些指标将是你的核心指导。\n\n3.  **充分利用 Nginx 的能力：** Nginx 作为一个高性能的 Web 服务器和反向代理，在现代系统架构中无处不在。除了作为基础的 Web 服务器，了解如何将其用作负载均衡器、缓存层和 SSL Termination (SSL 终结) 卸载器，将帮助你设计更健壮、可伸缩且安全的系统。例如，在微服务架构中，Nginx 可以作为 API Gateway 统一入口，进行请求转发、认证、限流和缓存。深入理解其配置和模块，可以解锁更多优化潜力。\n\n4.  **构建强大的网络故障诊断能力：** 文章列出的网络调试命令是工程师的“瑞士军刀”。在生产环境中，网络问题往往是最难诊断的。掌握这些命令（如 ping、traceroute、ss、dig、tcpdump 等）意味着你可以在不依赖他人或复杂工具的情况下，快速定位问题是出在网络连通性、路由、DNS 解析、端口监听还是流量传输上。这对于缩短 MTTR (平均恢复时间) 至关重要，让你能迅速响应并解决生产事故。\n\n5.  **巩固网络基础知识以设计更优架构：** Hub、Switch 和 Router 的分层解释巩固了你对网络设备和 OSI 模型 (特别是物理层、数据链路层和网络层) 的理解。理解 collision domain (冲突域) 和 broadcast domain (广播域) 的概念，能帮助你更好地设计网络拓扑、优化子网划分，并在排查网络瓶颈或隔离故障时做出更明智的决策。例如，在规划数据中心网络时，理解 Switch 如何在 LAN 内部高效转发数据，以及 Router 如何连接不同网络并隔离流量，是必不可少的基础。\n\n总的来说，这篇文章虽然内容简短，但覆盖了系统设计中多个核心领域的重要概念。掌握这些知识不仅能让你更好地理解现有系统的工作原理，更能为你在设计和构建未来系统时提供坚实的基础和实用工具，提升你解决复杂问题的能力。",
    "url": "https://blog.bytebytego.com/p/ep194-evolution-of-http"
  },
  {
    "id": "2025-12-21-ep194-evolution-of-http",
    "title": "EP194: Evolution of HTTP",
    "date": "2025-12-21",
    "preview": "EP194: HTTP 的演进 ByteByteGo 2025年12月20日  155 4 3 分享  ✂️ 使用 QA Wolf 将您的 QA 周期缩短至几分钟（赞助） 如果缓慢的 QA 流程阻碍了您或您的软件工程团队，并导致发布速度变慢——您需要了解一下 QA Wolf。 Q...",
    "content": "EP194: HTTP 的演进\nByteByteGo\n2025年12月20日\n\n155\n4\n3\n分享\n\n✂️ 使用 QA Wolf 将您的 QA 周期缩短至几分钟（赞助）\n如果缓慢的 QA 流程阻碍了您或您的软件工程团队，并导致发布速度变慢——您需要了解一下 QA Wolf。\nQA Wolf 的 AI 原生服务支持 Web 和移动应用程序，可在数周内实现 80% 的自动化测试覆盖率，并通过将 QA 周期缩短至几分钟来帮助团队将发布速度提高 5 倍。\nQA Wolf 将测试工作从您的任务清单中移除。他们可以为您提供：\n*   Web 和移动应用程序的无限并行测试运行\n*   24 小时维护和按需测试创建\n*   经人工验证的错误报告直接发送给您的团队\n*   零失误保证\n\n这样做的好处是？不再需要手动 E2E (端到端) 测试。不再有缓慢的 QA 周期。不再有错误到达生产环境。\n借助 QA Wolf，Drata 80 多名工程师的团队实现了 4 倍的测试用例增长和 86% 更快的 QA 周期。\n安排演示以了解更多信息\n\n本周的系统设计回顾：\n*   HTTP 的演进\n*   每位工程师都应该了解的系统性能指标\n*   Nginx 为何如此流行？\n*   每位工程师都应该了解的网络调试命令\n*   Hub、Switch 和 Router 解释\n\n赞助我们\n\nHTTP 的演进\n超文本传输协议 (HTTP) 多年来不断演进，以满足现代应用程序的需求，从简单的文本传输到高性能、实时体验。\n以下是 HTTP 的发展历程：\n*   **HTTP/0.9**: 专为通过单个 GET 请求获取简单的 HTML 文档而构建。\n*   **HTTP/1.0**: 添加了 Headers (头部) 和 Status codes (状态码) 以支持更丰富的交互，但每个请求仍然需要新的连接。\n*   **HTTP/1.1**: 引入了 Persistent connections (持久连接) 和更多方法，使 Web 在日常浏览中更快、更高效。\n*   **HTTP/2**: 通过 Multiplexing (多路复用) 解决了性能瓶颈，使多个请求能够共享一个连接。\n*   **HTTP/3 (QUIC)**: 转向使用基于 UDP 的 QUIC 协议，以减少延迟并提高可靠性，特别是对于移动和实时应用程序。\n\n轮到你了：您的项目中是否已经在使用 HTTP/3 了？\n\n2026 年 OKR：使用 AI 将 MTTR 提速 70% 并优化成本（赞助）\nClaude 编写的代码即将投入生产，但这不必是痛苦的。\nCoinbase、Toast、Gametime、MSCI 和 Zscaler 的工程团队使用 Resolve AI 来解决事件、优化成本，并利用跨代码、基础设施和遥测 (Telemetry) 的 AI 来构建具有生产环境背景的系统。\n结果意味着 MTTR (平均恢复时间) 提速 70%，每次事件所需的工程师数量减少 30%，并节省了数千个工程工时。想象一下，您可以在 2026 年利用这些时间交付什么？\n了解更多关于生产环境 AI、工作流自主多智能体系统，以及如何减少编排成本、改进调查并将工程时间从繁琐工作转向卓越工作。\n下载免费的生产环境 AI 电子书\n\n每位工程师都应该了解的系统性能指标\n您的 API 很慢。但到底有多慢？您需要数字。真实的指标才能告诉您到底哪里出了问题以及如何修复。\n以下是每位工程师在分析系统性能时应该了解的四个核心指标：\n*   **Queries Per Second (QPS)**: 您的系统每秒处理的传入请求数。您的服务器在一秒内收到 1,000 个请求？那就是 1,000 QPS。这听起来很简单，直到您意识到大多数系统无法长时间维持其峰值 QPS 而不出现问题。\n*   **Transactions Per Second (TPS)**: 您的系统每秒处理的已完成事务数。一个事务包括完整的往返过程，即请求发出，到达数据库，并带着响应返回。TPS 告诉您实际完成的工作量，而不仅仅是收到的请求。这是您的业务真正关心的。\n*   **Concurrency (并发数)**: 您的系统在任何给定时刻处理的同时活跃请求数。您可能每秒有 100 个请求，但如果每个请求需要 5 秒才能完成，那么您实际上同时处理着 500 个并发请求。高并发意味着您需要更多的资源、更好的连接池和更智能的线程管理。\n*   **Response Time (RT)**: 从请求开始到收到响应所经过的时间。在客户端和服务端都进行测量。\n\n一个简单的关系将它们联系在一起：QPS = Concurrency ÷ Average Response Time。\n更高的并发数或更低的响应时间 = 更高的吞吐量。\n\n轮到你了：当您分析性能时，您首先关注哪个指标，QPS、TPS 还是 Response Time？\n\nNginx 为何如此流行？\nApache 在 Web 服务器领域主导了 20 年，然后 Nginx 出现并改变了一切。现在 Nginx 为包括 Netflix、Airbnb、Dropbox 和 WordPress.com 在内的一些互联网上最大的网站提供支持。这并非因为它更新潮，而是因为它解决了 Apache 无法高效处理的问题。\n以下是 Nginx 如此受欢迎的原因：\n*   **高性能 Web 服务器**\n*   **反向代理 (Reverse Proxy) 和负载均衡器 (Load Balancer)**\n*   **缓存层 (Caching Layer)**\n*   **SSL 终止 (SSL Termination / Offloading)**\n\n轮到你了：您目前主要将 Nginx 用于什么，Web 服务器、反向代理还是负载均衡器？\n\n每位工程师都应该了解的网络调试命令\n当有人说“这是一个网络问题”时，这些命令可以帮助您快速找到问题所在。\n*   **ping**: 检查目标是否响应，并报告往返时间以确认基本可达性。\n*   **traceroute / tracert**: 显示路径上的每个跳点 (hop)，这样您就可以看到数据包在哪里变慢或停止。\n*   **mtr / pathping**: 持续测量每个跳点的延迟和丢包，以捕获间歇性问题。\n*   **ip addr, ip link / ipconfig /all**: 打印本地 IP 地址、MAC 地址和接口状态，以便您可以验证机器的网络身份。\n*   **ip route**: 显示路由表，以确认系统将使用哪个网关和下一个跳点。\n*   **ip neigh**: 显示 IP 到 MAC 地址的条目，以检测局域网 (LAN) 上的重复或陈旧的 ARP 记录。\n*   **ss -tulpn**: 列出监听套接字和 PID (进程ID)，以便您可以确认服务是否确实绑定到预期的端口。\n*   **dig**: 解析 DNS 记录，以验证客户端将连接到的确切 IP 地址。\n*   **curl -I**: 仅获取 HTTP(S) 头部，以检查状态码、重定向和缓存设置。\n*   **tcpdump / tshark**: 捕获数据包，以便您可以检查实际流量并验证发送和接收的内容。\n*   **iperf3**: 测量两个主机之间的端到端吞吐量，以区分带宽限制和应用程序问题。\n*   **ssh**: 在远程机器上打开安全 Shell，以直接运行检查和应用修复。\n*   **sftp**: 安全地传输文件，以便您可以在事件期间拉取日志或推送工件。\n*   **nmap**: 扫描开放端口和探测版本，以确认哪些服务已暴露并正在响应。\n\n轮到你了：在调试网络问题时，您首选的命令是什么？\n\nHub、Switch 和 Router 解释\n每个家庭和办公室网络都依赖这三款设备：Hub、Switch 和 Router，但它们的角色经常被混淆。\n*   **Hub (集线器)**: 运行在 OSI 模型的第一层 (物理层)。它是三者中最简单的，不理解地址或数据类型。当一个数据包到达时，它只是将其广播到所有连接的设备，创建一个大的冲突域 (collision domain)。这意味着所有设备都争夺相同的带宽，使得 Hub 在现代网络中效率低下。\n*   **Switch (交换机)**: 工作在 OSI 模型第二层 (数据链路层)。它学习 MAC 地址，并将数据帧 (frame) 只转发到正确的目的设备。Switch 上的每个端口都充当自己的冲突域，提高了局域网 (LAN) 内的效率和通信速度。\n*   **Router (路由器)**: 运行在 OSI 模型第三层 (网络层)。它根据 IP 地址路由数据包，并连接不同的网络，例如您的家庭网络到互联网。每个 Router 接口形成一个单独的广播域 (broadcast domain)，保持本地和外部流量的隔离。\n\n理解这三层如何协同工作是每个现代网络的基础，从您的家庭 Wi-Fi 到全球互联网骨干网。\n\n轮到你了：您通常如何判断网络问题是由 Router 还是 Switch 引起的？\n\n赞助我们\n让您的产品呈现在超过 1,000,000 名技术专业人士面前。\n我们的新闻简报将您的产品和服务直接呈现在一个重要的受众面前——数十万工程领导者和高级工程师——他们对重要的技术决策和重大采购具有影响力。\n广告位快速售罄 - 立即预订\n广告位通常提前约 4 周售罄。为确保您的广告能够触达这一有影响力的受众，请立即发送电子邮件至 sponsorship@bytebytego.com 预订您的位置。\n\n155\n4\n3\n分享\n\n---\n\n### 要点总结\n\n1.  **HTTP 的演进**: HTTP 协议从最初简单的文件传输 (HTTP/0.9) 逐步发展，引入了头部与状态码 (HTTP/1.0)，实现了持久连接 (HTTP/1.1)，通过多路复用解决性能瓶颈 (HTTP/2)，并最终转向基于 UDP 的 QUIC 协议以降低延迟和提高可靠性 (HTTP/3)。\n2.  **系统性能核心指标**: 工程师应关注 QPS (每秒查询数)、TPS (每秒事务数)、Concurrency (并发数) 和 Response Time (响应时间) 来分析系统性能，并理解它们之间的关系：QPS = Concurrency ÷ Average Response Time。\n3.  **Nginx 的多功能性**: Nginx 不仅仅是一个高性能的 Web 服务器，还广泛用作反向代理、负载均衡器、缓存层和 SSL 终止器，解决了传统 Web 服务器在处理高并发场景下的效率问题。\n4.  **网络调试常用命令**: 掌握 ping, traceroute, ip addr, ss, dig, curl -I, tcpdump, nmap 等一系列网络命令，是快速诊断和解决网络问题的关键。\n5.  **Hub、Switch、Router 的区别**: 理解 Hub (物理层，广播所有数据，单一冲突域)、Switch (数据链路层，智能转发，每个端口独立冲突域) 和 Router (网络层，基于 IP 路由，独立广播域) 在 OSI 模型中的工作层级和功能差异，是构建和排查网络故障的基础。\n\n### 你可以从这篇文章学到什么\n\n对于一位有几年经验的后端/系统设计工程师来说，这篇文章提供了一个极佳的“速查表”和知识点串联。它并非深入探讨每个主题的细节，而是用简洁明了的方式回顾了你在日常工作中可能会遇到的核心概念和实用工具。\n\n1.  **刷新基础知识**: 无论你有多资深，定期回顾基础知识都是必要的。这篇文章涵盖了 HTTP 协议的演进，可以帮助你理解当前主流的 HTTP/2 和 HTTP/3 为何出现、解决了什么问题，以及它们在实际项目中能带来哪些性能优势（例如，HTTP/2 的多路复用对于前端资源的加载优化，HTTP/3 在移动网络下的低延迟表现）。\n2.  **提升性能分析能力**: 对 QPS、TPS、并发和响应时间的理解是系统性能调优的基石。了解它们之间的相互关系（QPS = Concurrency ÷ Average Response Time）能帮助你更精准地定位系统瓶颈。例如，如果 QPS 降低而并发数升高，同时响应时间变长，你可能需要关注后端服务处理效率或数据库慢查询。这些指标是你构建监控仪表盘、设定告警阈值、进行容量规划和压测分析时的核心依据。\n3.  **掌握 Nginx 的应用场景**: Nginx 的流行并非偶然。除了作为静态资源服务器，其作为反向代理和负载均衡器的功能在微服务架构中至关重要。作为工程师，你应能熟练运用 Nginx 进行服务发现、流量分发、会话保持、缓存优化以及 SSL/TLS 卸载，从而提高服务的可用性、可扩展性和安全性。\n4.  **强化网络故障排除技能**: 在分布式系统中，“网络问题”是工程师常听到的“背锅侠”。这篇文章列出的网络调试命令是你在实际工作中快速定位问题的利器。掌握这些命令及其用途，能够让你在面对连接超时、路由不通、DNS 解析失败、端口未监听等问题时，从容不迫地进行诊断，大幅提高故障排除效率。\n5.  **加深对网络拓扑的理解**: 虽然 Hub、Switch 和 Router 看起来是入门级概念，但理解它们的工作原理和所在层级（L1、L2、L3）对于设计网络架构、隔离故障域以及理解容器网络、云网络 (VPC) 等高级概念至关重要。清楚地区分冲突域和广播域，能帮助你设计更高效、更稳定的网络环境。\n\n总的来说，这篇文章是一份实用的系统设计和运维指南，它将零散的知识点串联起来，让你能够系统性地思考和解决问题，从而在实际项目中做出更明智的技术决策，并更有效地应对各种挑战。",
    "url": "https://blog.bytebytego.com/p/ep194-evolution-of-http"
  },
  {
    "id": "2025-12-20-a-guide-to-retry-pattern-in-distributed-systems",
    "title": "A Guide to Retry Pattern in Distributed Systems",
    "date": "2025-12-20",
    "preview": "A Guide to Retry Pattern in Distributed Systems 分布式系统中的重试模式指南  ByteByteGo Dec 18, 2025 ∙ Paid 70 1 5 Share  在单体应用中，函数调用是一个本地的、内存中的进程。除了灾难性的硬...",
    "content": "A Guide to Retry Pattern in Distributed Systems\n分布式系统中的重试模式指南\n\nByteByteGo\nDec 18, 2025\n∙ Paid\n70\n1\n5\nShare\n\n在单体应用中，函数调用是一个本地的、内存中的进程。除了灾难性的硬件故障或进程崩溃之外，函数的执行基本得到保证。如果进程存活，调用就会成功。\n\n然而，在分布式系统中，这种保证不再成立。组件通过物理网络进行通信，而物理网络本质上是不可靠的。这一现实体现在“分布式计算的谬误”（Fallacies of Distributed Computing）中，特别是第一条谬误：“网络是可靠的”。但事实上，它并非如此。从 Service A 发送到 Service B 的请求可能会失败，并不是因为 Service B 本身出现故障，而仅仅是因为通信介质暂时出现了问题。\n\n这催生了对防御性编程模式的需求，其中我们使用的一个主要机制就是**重试模式**（Retry pattern）。通过自动重试失败的操作，系统可以牺牲一定的延迟（latency）来换取可用性（availability），从而将一个原本会失败的用户请求转化为成功的请求。\n\n然而，在分布式系统中，重试（retries）既至关重要又充满危险。一方面，它们能将不可靠的网络转化为相对可靠的网络。但另一方面，不加区别的重试可能导致延迟放大（latency amplification）、资源耗尽（resource exhaustion）以及级联故障（cascading failures），甚至可能导致整个平台崩溃。\n\n在本文中，我们将深入探讨重试模式，了解何时以及如何安全有效地使用它。\n\n什么是重试？\n\n继续免费阅读此文章，由 Alex Xu 友情提供。\n\n领取我的免费文章\n\n或购买付费订阅。\n\n## 要点总结\n\n*   **分布式系统中的网络不可靠性**：与单体应用不同，分布式系统中的网络通信本质上不可靠，请求可能会因网络瞬时故障而失败。\n*   **重试模式的引入**：Retry pattern（重试模式）是一种防御性编程模式，旨在通过自动重试失败的操作来提高系统的可用性。\n*   **权衡与收益**：通过重试，系统可以牺牲一定的延迟（latency）来提高可用性（availability），将原本失败的请求转化为成功。\n*   **重试的危险性**：不加区别的重试可能导致延迟放大（latency amplification）、资源耗尽（resource exhaustion）和级联故障（cascading failures），甚至可能导致整个平台崩溃。\n*   **核心议题**：文章将深入探讨何时以及如何安全有效地使用重试模式。\n\n## 你可以从这篇文章学到什么\n\n作为一名资深的后端或系统设计工程师，你可能已经无数次遇到过因为网络波动或瞬时服务故障导致的请求失败。这篇文章虽然只是一个引子，但它精准地抓住了分布式系统中最核心、也最容易被忽视的问题——网络的本质不可靠性，并提出了一个关键的应对策略：Retry pattern（重试模式）。\n\n你可以从这篇文章中学到以下几点，并将其应用于实际项目：\n\n1.  **对分布式系统网络本质的深刻理解**：文章开篇便强调了“网络是不可靠的”这一分布式计算的基本谬误。这提醒我们，在设计分布式系统时，必须从根本上承认并处理网络通信的易变性。理解这一点是构建高可用、高韧性系统的基石，避免将本地函数调用的思维套用到远程服务调用上。\n2.  **重试模式的价值与局限性**：文章指出了 Retry pattern 在提升系统可用性方面的巨大价值——它能将瞬时故障“抹平”，将失败的请求转化为成功。然而，更重要的是它提前警告了重试的“危险性”。对于有经验的工程师而言，这并非简单的概念普及，而是对实践中可能遇到的真实问题的预警，例如：\n    *   **延迟放大**：不恰当的重试策略（如固定间隔、重试次数过多）可能导致用户请求响应时间大幅增加。\n    *   **资源耗尽**：当后端服务压力过大或出现故障时，大量的重试请求可能进一步压垮服务，消耗连接池、线程等资源。\n    *   **级联故障**：一个服务的重试行为可能像多米诺骨牌一样，将负载传递并放大到其依赖服务，最终导致整个系统崩溃。\n3.  **如何将这些理念应用于实际项目**：\n    *   **设计合理的重试策略**：在你的微服务、API网关、数据库客户端或消息队列消费者中实现重试机制时，应仔细考虑重试次数上限、重试间隔（例如采用指数退避算法，避免“惊群效应”）、以及抖动（jitter）来分散请求。\n    *   **区分可重试与不可重试错误**：并非所有错误都应重试。例如，HTTP 5xx 错误（服务器内部错误）、网络超时通常是可重试的；而 HTTP 4xx 错误（客户端错误，如认证失败、参数错误）或明确的业务逻辑错误则不应重试。你需要根据错误类型和业务幂等性来做决策。\n    *   **结合断路器（Circuit Breaker）模式**：为了防止重试导致级联故障，应将 Retry pattern 与 Circuit Breaker 模式结合使用。当服务持续失败时，断路器可以“跳闸”，直接拒绝请求，给下游服务恢复的时间，而不是盲目重试。\n    *   **完善监控与告警**：在系统中引入重试机制后，务必对重试次数、重试成功率、因重试导致的平均延迟以及下游服务的错误率进行监控和告警。这能帮助你及时发现重试是否在过度工作，或者是否存在潜在的、被重试掩盖的慢性问题。\n    *   **考虑幂等性**：对于涉及数据修改的操作，确保你的操作是幂等的至关重要。即使重试导致请求发送多次，也能保证系统状态的一致性，避免副作用。\n\n总而言之，这篇文章虽然短小，但它为你在分布式系统中设计和实现健壮的重试机制提供了一个重要的视角——重试并非万能药，它是一把双刃剑。作为工程师，你需要深入理解其原理，并在实际项目中谨慎、智慧地运用它。",
    "url": "https://blog.bytebytego.com/p/a-guide-to-retry-pattern-in-distributed"
  },
  {
    "id": "2025-12-19-a-guide-to-retry-pattern-in-distributed-systems",
    "title": "A Guide to Retry Pattern in Distributed Systems",
    "date": "2025-12-19",
    "preview": "# 分布式系统中的重试模式指南  ByteByteGo 2025年12月18日 ∙ 付费 57 1 5 分享  在单体应用（monolithic application）中，函数调用（function call）是一个本地的、在内存中执行的进程。除了灾难性的硬件故障或进程崩溃，函...",
    "content": "# 分布式系统中的重试模式指南\n\nByteByteGo\n2025年12月18日\n∙ 付费\n57\n1\n5\n分享\n\n在单体应用（monolithic application）中，函数调用（function call）是一个本地的、在内存中执行的进程。除了灾难性的硬件故障或进程崩溃，函数执行的成功几乎是板上钉钉的。如果进程活着，调用就会成功。\n\n然而，在分布式系统（distributed systems）中，这种保证就不复存在了。组件之间通过物理网络进行通信，而物理网络本质上是不可靠的。这一现实在“分布式计算的谬误”（Fallacies of Distributed Computing）中得到了体现，特别是第一个谬误：“网络是可靠的”（The network is reliable）。事实上，它并非如此。从服务 A（Service A）发送到服务 B（Service B）的请求可能会失败，并不是因为服务 B 自身有问题，而仅仅是因为通信介质（communication medium）暂时出现了故障。\n\n这催生了对防御性编程模式（defensive programming patterns）的需求，而我们使用的主要机制之一就是重试模式（Retry pattern）。通过自动重试失败的操作，系统可以“以延迟换取可用性”（trade latency for availability），从而将原本失败的用户请求转化为成功的请求。\n\n然而，在分布式系统中，重试（retries）既至关重要又充满危险。一方面，它们能将不可靠的网络转化为相对可靠的网络。但另一方面，不加区分的重试可能导致延迟放大（latency amplification）、资源耗尽（resource exhaustion）以及级联故障（cascading failures），最终可能使整个平台崩溃。\n\n在本文中，我们将深入探讨重试模式，理解何时以及如何安全有效地使用它。\n\n什么是重试（Retry）？\n\n通过7天免费试用继续阅读\n订阅 ByteByteGo Newsletter\n继续阅读本文并获得7天免费访问所有历史文章的权限。\n开始试用\n已是付费订阅者？\n登录\n\n---\n\n### 要点总结\n\n*   在单体应用中，函数调用具有很高的可靠性；但在分布式系统中，由于网络本质上不可靠，这种可靠性不复存在。\n*   “网络是可靠的”是分布式计算的第一大谬误，实际上，网络通信故障是常态。\n*   重试模式（Retry pattern）是一种重要的防御性编程机制，用于处理分布式系统中可能出现的瞬时故障。\n*   通过自动重试失败的操作，系统可以提高可用性，将原本失败的请求转化为成功，但这通常会增加请求的延迟。\n*   重试在提高系统可靠性的同时，也潜藏着巨大风险。\n*   不加区分的重试可能导致延迟放大（latency amplification）、系统资源耗尽（resource exhaustion），甚至引发级联故障（cascading failures），最终拖垮整个平台。\n*   理解何时以及如何安全有效地应用重试模式，是构建健壮分布式系统的关键。\n\n---\n\n### 你可以从这篇文章学到什么\n\n作为一名有几年经验的后端/系统设计工程师，你将从本文中获得以下重要洞察和实践指导：\n\n1.  **强化对分布式系统本质的理解**：文章开宗明义地指出，分布式系统与单体应用在网络可靠性上的根本差异。这提醒你在设计系统时，必须始终将“网络不可靠”作为基本假设，并据此构建容错机制，而不是将其视为偶发事件。\n2.  **认识重试模式的双面性**：你将了解到，Retry pattern（重试模式）是解决瞬时网络故障和提高系统可用性的关键工具。但更重要的是，文章强调了不加区分的重试所带来的巨大风险，如延迟放大、资源耗尽和级联故障。这能帮助你避免在实际项目中盲目引入重试机制，并促使你深入思考其潜在的副作用。\n3.  **系统设计中的防御性思维**：本文引入了“防御性编程模式”的概念，促使你在设计API、服务间通信及容错机制时，主动思考并预设各种失败场景，并提前规划应对策略。这有助于你构建更稳定、更具弹性的系统。\n4.  **实践应用指导**：\n    *   在考虑引入重试时，要深入思考其**必要性**和**潜在危害**。例如，对于幂等（idempotent）操作，重试通常是安全的；而对于非幂等操作，则需配合其他补偿机制或严格限制。\n    *   理解重试并非万能药，它需要结合**熔断（Circuit Breaker）**、**限流（Rate Limiting）**、**超时（Timeout）**等其他容错模式一同使用，才能构建出真正健壮的分布式系统。\n    *   关注重试策略的**参数配置**，如最大重试次数、重试间隔（backoff strategy，如指数退避）以及哪些错误码应该重试。不合理的配置会直接导致文中提到的风险。\n    *   在部署系统后，要密切**监控**重试行为和其对系统整体性能（如延迟、资源利用率）的影响，以便及时调整优化。",
    "url": "https://blog.bytebytego.com/p/a-guide-to-retry-pattern-in-distributed"
  },
  {
    "id": "2025-12-18-how-meta-built-a-new-ai-powered-ads-model-for-5-better-conversions",
    "title": "How Meta Built a New AI-Powered Ads Model for 5% Better Conversions",
    "date": "2025-12-18",
    "preview": "Meta 如何构建全新 AI 驱动的广告模型，实现 5% 的转化率提升 ByteByteGo 2025年12月17日 95 2 分享 将代码审查时间和错误减少一半（赞助内容） 代码审查至关重要但耗时。CodeRabbit 可作为您的 AI 副驾驶，为每次拉取请求提供即时代码审查评...",
    "content": "Meta 如何构建全新 AI 驱动的广告模型，实现 5% 的转化率提升\nByteByteGo\n2025年12月17日\n95\n2\n分享\n将代码审查时间和错误减少一半（赞助内容）\n代码审查至关重要但耗时。CodeRabbit 可作为您的 AI 副驾驶，为每次拉取请求提供即时代码审查评论和潜在影响。\n除了标记问题，CodeRabbit 还提供一键修复建议，并允许您使用 AST Grep 模式定义自定义代码质量规则，从而捕获传统静态分析工具可能遗漏的细微问题。\nCodeRabbit 迄今已审查了超过 1000 万个 PR，安装在 200 万个存储库上，并被 10 万个开源项目使用。CodeRabbit 对所有开源存储库免费。\n立即开始\n免责声明：本文中的细节来源于 Meta 工程团队在线分享的信息。所有技术细节的功劳归 Meta 工程团队所有。原文和参考资料的链接在文章末尾的参考文献部分。我们尝试分析了这些细节并提供了我们的看法。如果您发现任何不准确或遗漏之处，请留言，我们将尽力修正。\n\nMeta 在 2025 年第二季度宣布，其新的生成式广告模型（Generative Ads Model，简称 GEM）使 Instagram 的广告转化率提高了 5%，Facebook Feed 的转化率提高了 3%。这些数字可能看起来不大。\n\n然而，在 Meta 的规模下，这些百分比意味着数十亿美元的额外收入，并代表着 AI 驱动广告运作方式的根本性转变。\n\nGEM 是有史以来为推荐系统构建的最大基础模型。它的训练规模通常仅限于 GPT-4 或 Claude 等大型语言模型（LLMs）。但矛盾之处在于：GEM 如此强大且计算密集，以至于 Meta 实际上无法直接用它来向用户投放广告。\n\n相反，该公司开发了一种教师-学生架构（teacher-student architecture），让更小、更快的模型能够受益于 GEM 的智能，而无需承担其计算成本。\n\n在本文中，我们将探讨 Meta 工程团队如何构建 GEM 以及他们克服的挑战。\n\n👋 告别低测试覆盖率和缓慢的 QA 周期（赞助内容）\n当少于 80% 的用户流程在发布前未经测试时，Bug 就会悄然出现。然而，对于任何团队来说，实现这种覆盖率（并保持下去）既困难又昂贵。\nQA Wolf 的 AI 原生解决方案为 Web 和移动应用提供大容量、高速的测试覆盖，将您的组织的 QA 周期缩短至几分钟。\n他们可以为您提供：\n在数周而非数年内实现 80% 的自动化端到端（E2E）测试覆盖\n无限并行测试运行\n24 小时维护和按需测试创建\n零缺陷保证\n好处是什么？不再进行手动 E2E 测试。不再有缓慢的 QA 周期。不再有 Bug 进入生产环境。\n在 QA Wolf 的帮助下，Drata 的工程师团队实现了 4 倍的测试用例和 86% 更快的 QA 周期。\n⭐ 在 G2 上评分为 4.8/5\n安排演示以了解更多信息\n\nGEM 解决的核心问题\n每天，数十亿用户在 Facebook、Instagram 和其他 Meta 平台上滚动浏览，产生数万亿潜在的广告展示机会。每一次展示都代表一个决策点：从数百万种可能性中，应该在特定时刻向特定用户展示哪个广告？弄错了意味着在不相关的广告上浪费广告商的预算，并用用户不关心的内容惹恼用户。做对了则为所有相关方创造价值。\n\n传统的广告推荐系统在以下几个方面难以应对：\n一些系统将每个平台单独处理，这意味着关于用户在 Instagram 上行为的洞察无法用于 Facebook 上的预测。这种孤立的方法错过了有价值的跨平台模式。其他系统试图将所有平台视为相同，忽略了人们与 Instagram Stories 的互动方式与他们浏览 Facebook Feed 的方式截然不同。这两种方法都不是最优的。\n\n数据复杂性也以以下方式加剧了这些挑战：\n*   与总展示量相比，点击和转化等有意义的信号极其稀疏。\n*   用户特征是动态的，并且不断变化。\n*   系统必须处理多模态输入，包括文本、图像、视频和复杂的行为序列。\n*   传统模型存在严重的内存限制，通常只考虑用户的最近 10 到 20 次操作。\n\nGEM 的目标是创建一个统一的智能系统，在 Meta 的整个生态系统中全面理解用户，从长期的行为历史和复杂的跨平台模式中学习，同时保持针对每个特定界面和目标进行优化的细微差别。\n\nGEM 如何理解用户？\nGEM 的架构通过三个互补的系统处理用户和广告信息，每个系统处理预测问题的不同方面。\n\n第一个系统处理 Meta 称之为“非序列特征”（non-sequence features）的信息，这些信息本质上是静态属性及其组合。这包括用户的年龄和位置等人口统计数据、用户兴趣、广告的格式和创意内容等广告特征，以及广告商的目标。\n\n这里的挑战不仅在于了解这些个体特征，还在于理解它们如何相互作用。例如，一个 25 岁的技术工作者与一个 25 岁的教师有着截然不同的购买模式，即使他们有一些共同的兴趣。系统需要学习哪些特征组合是真正重要的。\n\nGEM 使用 Wukong 架构的增强版本，该架构具有可堆叠分解机（stackable factorization machines），可以在更深层次的交互方面纵向扩展，也可以在更广泛的特征覆盖方面横向扩展。该架构通过多个堆叠层工作，每个后续层都从前一层发现的更简单模式中学习越来越复杂的模式。例如，早期层可能会发现年轻专业人士对科技产品广告反应良好这一基本模式。堆栈中更深一层则在此基础上学习，发现居住在城市地区并对健身表现出兴趣的年轻专业人士对智能穿戴设备广告反应特别好。更深一层可能会进一步细化，发现这种组合在广告强调数据追踪功能而非时尚元素时效果最佳。\n\n第二个系统处理“序列特征”（sequence features），它捕捉用户行为的时间线。用户的行为并非孤立存在。它们通过顺序和意义讲述一个故事。一个点击了居家健身内容，然后搜索附近健身房，然后查看了几个健身房网站，然后研究了会员费用的用户，显然正在经历一个特定的旅程。传统架构难以高效处理长序列，因为计算成本随序列长度迅速增长。\n\nGEM 通过金字塔并行结构（pyramid-parallel structure）克服了这一问题。可以将其视为在底层以块状处理您的行为历史，然后在中间层将这些块组合成更广泛的模式，最后在顶层将所有内容综合为完整的旅程理解。多个块可以同时而不是顺序处理，这显著提高了效率。\n\n这里的突破是规模。GEM 现在可以分析您过去的数千次操作，而不仅仅是最近的几次。这种扩展的视角揭示了更短的时间窗口根本无法捕捉到的模式，例如从偶然兴趣到认真购买意图的进展，这可能需要数月的时间才能形成。\n\n请看下图：\n[此处应为图片，文本版省略]\n\n第三个系统，名为 InterFormer，通过连接您的静态个人资料和您的行为时间线来处理“跨特征学习”（cross-feature learning）。这是 GEM 智能真正显现的地方。以前的方法会将您的整个行为历史压缩成一个紧凑的摘要向量（就像将一部完整的小说缩减为单一评分）。这种压缩不可避免地会丢失关于您旅程的关键细节。\n\nInterFormer 采用交错结构（interleaving structure）的不同方法。它在纯粹专注于理解您的行为序列的层和将这些行为与您的个人资料属性连接起来的层之间交替。\n\n第一个序列层可能识别出您对健身的兴趣随时间增加。\n第一个跨特征层然后考虑您的年龄、收入和位置背景如何塑造这种健身兴趣的含义。\n第二个序列层用这些新洞察重新审视您的行为，并可能注意到在您工作地点附近开了一家健身房后，您的健身研究加剧了。\n第二个跨特征层然后对购买意图和时机做出更深层次的连接。\n\n这种交替过程通过多个层持续进行，每个周期都在不丢失对完整行为记录访问的情况下提炼理解。\n\n使用 GEM 面临的实际问题\n尽管 GEM 具有明显的优势，但 Meta 在使用 GEM 方面面临着一个根本性的工程挑战。\n\nGEM 体积庞大，使用数千个 GPU 进行了长时间训练。直接为每次广告预测运行 GEM 将是慢得令人无法接受且成本高昂的。当用户在 Instagram 上滚动时，系统需要在几十毫秒内做出广告决策。GEM 根本无法在同时服务数十亿用户的情况下以这种速度运行。\n\nMeta 的解决方案是一种教师-学生架构，其中 GEM 扮演着主导教师的角色，训练数百个更小、更快的“垂直模型”（Vertical Models，简称 VMs），这些模型才真正在生产中投放广告。这些 VM 专门用于特定场景，例如 Instagram Stories 的点击预测或 Facebook Feed 的转化预测。每个 VM 都足够轻量，可以在几毫秒内做出预测，但它们比独立训练时要智能得多，因为它们从 GEM 中学习。\n\n知识转移通过两种策略发生。当 VM 在与 GEM 训练的相同领域运行，具有相似数据和目标时，直接转移（Direct transfer）起作用。GEM 可以直接教授这些模型。当 VM 在与 GEM 训练领域截然不同的专业领域工作时，分层转移（Hierarchical transfer）适用。在这些情况下，GEM 首先为 Instagram 或 Facebook Marketplace 等领域训练中等大小的领域特定基础模型（domain-specific foundation models）。然后，这些领域模型再教授更小的 VM。知识通过层级向下流动，在每个阶段进行适应和专门化。\n\nMeta 采用了三种先进技术来最大化转移效率：\n*   **带有学生适配器（Student Adapter）的知识蒸馏（Knowledge distillation）：** 学生模型学习复制 GEM 的推理过程，而不仅仅是最终预测。Student Adapter 使用最近的真实数据精炼 GEM 的预测，调整时间延迟和领域特定差异。\n*   **表征学习（Representation learning）：** 在教师模型和学生模型之间创建共享的概念框架。GEM 学习以能很好地跨不同模型大小进行转移的方式编码信息，在广告服务期间不增加计算开销。\n*   **参数共享（Parameter sharing）：** 这允许 VM 选择性地直接整合 GEM 中的特定组件。小型 VM 保持快速，同时借用 GEM 的复杂组件来执行复杂的用户理解任务。\n\n这三种技术结合起来，达到了标准知识蒸馏单独使用时的两倍效果。持续改进循环的工作方式如下：\n用户实时与快速 VM 互动\n他们的参与数据流回 Meta 的数据管道\nGEM 定期用这些新数据进行再训练，更新后的知识通过后训练技术转移到 VM\n改进后的 VM 部署到生产环境。\n这个循环持续重复，GEM 变得更智能，VM 定期获得智能更新。\n\n前所未有的规模训练\n构建 GEM 需要 Meta 从头开始重建其训练基础设施。\n\n挑战在于以 LLM 规模训练模型，但任务根本不同，是推荐而非语言生成。该公司实现了 23 倍的有效训练吞吐量提升，同时使用了 16 倍的 GPU，并同时将硬件效率提高了 1.43 倍。\n\n这需要多个领域的创新。多维并行（Multi-dimensional parallelism）协调数千个 GPU 如何协同工作，使用混合分片分布式并行（Hybrid Sharded Distributed Parallel）等技术分割模型的密集组件，同时通过数据并行和模型并行的组合处理嵌入表（embedding tables）等稀疏组件。目标是确保每个 GPU 都保持忙碌，并最大限度地减少等待来自其他 GPU 通信的空闲时间。\n\n系统级优化进一步提高了 GPU 利用率：\n*   为可变长度用户序列设计的自定义 GPU 内核（GPU kernels），融合操作以减少内存带宽瓶颈。\n*   PyTorch 2.0 图级编译（graph-level compilation）自动化激活检查点（activation checkpointing）和操作符融合（operator fusion）等优化。\n*   内存压缩（Memory compression），包括 FP8 量化（FP8 quantization），以在不影响准确性的情况下减少内存占用。\n*   NCCLX 通信集合（NCCLX communication collectives）处理 GPU 间通信，而不消耗主要的计算资源。\n\n效率提升不仅仅是原始训练速度。\nMeta 通过优化训练器初始化、数据读取器设置和检查点，将作业启动时间缩短了 5 倍。他们通过智能缓存策略将 PyTorch 2.0 编译时间缩短了 7 倍。这些可能看起来是微小的细节，但当您训练的模型需要花费数百万美元的计算资源时，每个百分点的效率提升都意义重大。\n\n结果是一个训练系统，可以快速迭代 GEM，以以前基础设施无法实现的速度整合新数据和架构改进。这使 Meta 能够将 GEM 保持在推荐 AI 的前沿，同时控制成本，使得这项巨大投资物有所值。\n\n结论\nMeta 的 GEM 路线图远超其当前能力。\n\n下一个主要演进涉及真正的多模态学习（multimodal learning），其中 GEM 同时处理文本、图像、音频和视频，而不是将它们视为单独的输入流。这将使 GEM 对用户偏好和广告创意效果在所有内容类型上都有更丰富的理解。该公司还在探索推理时扩缩（inference-time scaling），这将允许系统动态地为困难的预测分配更多计算资源，同时更有效地处理简单情况。\n\n或许最雄心勃勃的是，Meta 设想了一个统一互动模型（unified engagement model），该模型使用相同的底层智能对有机内容和广告进行排名。这将从根本上改变广告如何融入社交信息流，潜在地创造更无缝的体验，使广告感觉更像是自然的内容推荐而不是干扰。在广告商方面，GEM 的智能将实现更复杂的智能体自动化（agentic automation），其中 AI 系统可以以最少的人工干预管理和优化广告活动，同时实现更好的结果。\n\n参考文献：\nMeta’s Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation\nInterFormer Research Paper\nWukong: Towards a Scaling Law for Large-Scale Recommendation\n\n赞助我们\n让您的产品呈现在超过 1,000,000 名科技专业人士面前。\n我们的时事通讯将您的产品和服务直接展示给重要的受众——数十万工程领导者和高级工程师——他们对重要的技术决策和大额采购具有影响力。\n席位预订迅速——立即预订\n广告位通常提前约 4 周售罄。为确保您的广告能够触达这批有影响力的受众，请立即发送电子邮件至 sponsorship@bytebytego.com 预订您的席位。\n95\n2\n分享\n\n---\n\n### 要点总结\n\n1.  **显著的商业价值：** Meta 的生成式广告模型（GEM）在 Instagram 和 Facebook 上分别带来了 5% 和 3% 的广告转化率提升，转化为数十亿美元的额外收入。\n2.  **规模与性能的矛盾：** GEM 是目前最大的推荐系统基础模型，训练规模与大型语言模型（LLM）相当，但其计算开销过大，无法直接用于实时广告服务。\n3.  **教师-学生架构：** Meta 采用教师-学生架构（teacher-student architecture），由功能强大的 GEM（教师模型）训练数百个更小、更快的垂直模型（VMs，学生模型），用于生产环境中的实时广告投放。\n4.  **GEM 的多维用户理解：** GEM 通过处理非序列特征（静态属性）、序列特征（用户行为时间线）以及 InterFormer 实现的跨特征学习（连接静态档案与行为时间线），全面理解用户。\n5.  **应对复杂数据挑战：** GEM 解决了传统广告系统面临的平台孤岛、数据稀疏、动态用户特征、多模态输入以及内存限制（能分析用户数千次历史行为）等问题。\n6.  **高效知识转移策略：** 通过知识蒸馏（Knowledge distillation）与学生适配器（Student Adapter）、表征学习（Representation learning）和参数共享（Parameter sharing）三种技术，最大化 GEM 向 VM 的知识转移效率。\n7.  **持续改进的反馈循环：** 系统设计包含一个持续循环：用户互动数据回流 -> GEM 定期使用新数据再训练 -> 更新知识传递给 VM -> 改进后的 VM 部署到生产环境。\n8.  **前所未有的训练规模：** Meta 重建了训练基础设施，实现了 23 倍的有效训练吞吐量提升和 1.43 倍的硬件效率提升，得益于多维并行、自定义 GPU 内核、PyTorch 2.0 优化和 FP8 量化等创新。\n9.  **未来发展路线图：** GEM 的未来计划包括真正的多模态学习、推理时扩缩（inference-time scaling）以及一个统一互动模型（unified engagement model），用于同时排名有机内容和广告，并实现更高级的智能体自动化（agentic automation）。\n\n### 你可以从这篇文章学到什么\n\n对于具有几年经验的后端/系统设计工程师而言，这篇文章提供了宝贵的见解和可应用的设计模式：\n\n1.  **大规模模型部署的教师-学生架构（Teacher-Student Architecture）：** 这是处理大型、计算密集型 AI 模型上线部署的关键模式。当你拥有一个准确但运行成本高昂的模型（教师）时，可以利用它来训练多个小巧、快速的模型（学生），用于应对生产环境的实时请求。\n    *   **应用场景：** 如果你的数据科学团队开发出 SOTA（State-Of-The-Art）模型，但其推理延迟无法满足实时 API 调用需求，可以考虑知识蒸馏策略。教师模型可以离线运行或不频繁更新，而学生模型则负责处理实时流量，在保证性能的同时，继承教师模型的智能。这不仅限于广告推荐，也可用于大规模内容审核、个性化搜索等。\n2.  **处理复杂和多模态数据的系统设计：** 文章详细介绍了 GEM 如何整合静态特征、序列行为和跨特征交互。对于系统设计师来说，这强调了构建一个健壮的特征存储（feature store）和数据管道的重要性，该管道需要高效地摄取、转换并服务各种数据类型（结构化数据、时间序列数据、多模态数据），以喂给复杂的模型。\n    *   **应用场景：** 在设计机器学习数据管道时，不仅要考虑原始数据，还要思考如何有效地整合用户画像、历史交互和上下文信息。例如，如何存储和快速检索一个用户“数千次过去的行为”而不造成性能瓶颈，可以考虑使用时间序列数据库、图数据库或优化的列式存储。\n3.  **大规模 ML 训练基础设施的优化：** Meta 在训练基础设施上的创新（如多维并行、GPU 利用率优化、PyTorch 2.0 特性、内存压缩和通信集合）对任何大规模机器学习项目都具有指导意义。即使你没有构建 LLM 规模的系统，理解这些概念也能帮助你设计更高效的训练环境，显著降低成本和时间。\n    *   **应用场景：** 当你在处理大型模型或数据集时，应该了解数据并行、模型并行、混合精度训练（如 FP8 量化）和高效的 GPU 间通信（如 NCCLX）等技术。这些是优化计算资源利用率、减少模型训练时间与成本的关键。\n4.  **持续学习与反馈循环的设计：** 文章中描绘的用户互动 -> 数据回流 -> GEM 再训练 -> VM 更新 -> 部署的连续改进循环，是一个强大的设计模式。系统设计师应始终考虑如何将生产环境中的反馈数据高效地重新注入模型训练流程，实现模型的持续学习和适应。\n    *   **应用场景：** 在设计你的机器学习系统时，明确定义数据采集、清洗、特征工程、模型训练、模型评估和部署的反馈闭环。思考如何自动化这些流程，以及如何实现模型的 A/B 测试和灰度发布，确保新模型迭代能够稳定且快速地投入生产。\n5.  **构建统一智能层的思路：** Meta 设想的“统一互动模型”，使用同一套底层智能来排名有机内容和广告，展示了一种更宏观、更统一的系统设计趋势。\n    *   **应用场景：** 如果你的平台涉及多种内容类型（例如，社交媒体信息流、电商推荐、搜索结果），可以思考如何抽象出一个共享的“理解”层或“意图”层，为不同的子系统提供服务。这不仅可能带来更连贯、更个性化的用户体验，也能减少不同系统间的重复开发和维护成本。",
    "url": "https://blog.bytebytego.com/p/how-meta-built-a-new-ai-powered-ads"
  },
  {
    "id": "2025-12-17-how-meta-built-a-new-ai-powered-ads-model-for-5-better-conversions",
    "title": "How Meta Built a New AI-Powered Ads Model for 5% Better Conversions",
    "date": "2025-12-17",
    "preview": "Meta 如何构建新的 AI 驱动广告模型以实现 5% 的转化率提升 ByteByteGo  当 Meta 在 2025 年第二季度宣布其新的生成式广告模型 (GEM) 使 Instagram 的广告转化率提高了 5%，Facebook Feed 提高了 3% 时，这些数字可能看...",
    "content": "Meta 如何构建新的 AI 驱动广告模型以实现 5% 的转化率提升\nByteByteGo\n\n当 Meta 在 2025 年第二季度宣布其新的生成式广告模型 (GEM) 使 Instagram 的广告转化率提高了 5%，Facebook Feed 提高了 3% 时，这些数字可能看起来不大。\n\n然而，在 Meta 的规模下，这些百分比意味着数十亿美元的额外收入，并代表了 AI 驱动广告运作方式的根本性转变。\n\nGEM 是有史以来为推荐系统构建的最大基础模型。它的训练规模通常只用于像 GPT-4 或 Claude 这样的大型语言模型。然而，悖论在于：GEM 如此强大且计算密集，以至于 Meta 实际上无法直接使用它来向用户提供广告。\n\n相反，该公司开发了一种教师-学生架构，让更小、更快的模型能够从 GEM 的智能中获益，而无需继承其计算成本。\n\n在本文中，我们将探讨 Meta 工程团队如何构建 GEM 以及他们克服的挑战。\n\n## GEM 解决的核心问题\n\n每天，数十亿用户滚动浏览 Facebook、Instagram 和其他 Meta 平台，产生数万亿潜在的广告展示机会。每一次展示都代表一个决策点：从数百万种可能性中，应该在特定时刻向特定用户展示哪个广告？做错了，就意味着浪费广告商预算在不相关的广告上，并用用户不关心的内容惹恼他们。做对了，就能为所有相关方创造价值。\n\n传统的广告推荐系统在几个方面都面临挑战。一些系统将每个平台独立对待，这意味着关于用户在 Instagram 上行为的洞察无法用于 Facebook 上的预测。这种孤立的方法错失了有价值的跨平台模式。其他系统试图将所有平台同等对待，却忽略了人们与 Instagram Stories 互动的方式与他们浏览 Facebook Feed 的方式截然不同。这两种方法都不是最优的。\n\n数据复杂性也通过以下方式加剧了这些挑战：\n\n*   与总展示量相比，点击和转化等有意义的信号极其稀疏。\n*   用户特征是动态且不断变化的。\n*   系统必须处理多模态输入，包括文本、图像、视频和复杂的行为序列。\n*   传统模型存在严重的内存限制，通常只考虑用户最近 10 到 20 个行为。\n\nGEM 的目标是创建一个统一智能，在 Meta 的整个生态系统中全面理解用户，从长期的行为历史和复杂的跨平台模式中学习，同时保持针对每个具体界面和目标进行优化所需的细微差别。\n\n## GEM 如何理解用户？\n\nGEM 的架构通过三个互补系统处理用户和广告信息，每个系统处理预测问题的不同方面。\n\n第一个系统处理 Meta 所谓的非序列特征，这本质上是静态属性及其组合。这些包括用户人口统计信息（如年龄和位置）、用户兴趣、广告特征（如格式和创意内容）以及广告商目标。\n\n这里的挑战不仅在于了解这些单独的特征，还在于理解它们如何相互作用。例如，一个 25 岁的科技工作者与一个 25 岁的教师有非常不同的购买模式，即使他们有一些共同的兴趣。系统需要学习哪些特征组合真正重要。\n\nGEM 使用 Wukong 架构的增强版，该架构带有可堆叠因子分解机 (stackable factorization machines)，可以垂直扩展以实现更深层次的交互，也可以水平扩展以实现更广泛的特征覆盖。该架构通过多个堆叠层工作，其中每个后续层从前一层发现的更简单模式中学习日益复杂的模式。例如，早期层可能发现年轻专业人士对科技产品广告反应良好这一基本模式。堆栈中更深一层则在此基础上学习到，城市地区对健身感兴趣的年轻专业人士对智能可穿戴设备广告反应尤其好。更深一层可能会进一步细化这一点，发现这种组合在广告强调数据跟踪功能而非时尚元素时效果最佳。\n\n第二个系统处理序列特征，它捕捉用户行为的时间线。用户的行为并非孤立存在，它们以顺序和意义讲述一个故事。一个用户先点击了家庭健身内容，然后搜索附近的健身房，接着浏览了几个健身房网站，然后研究了会员费，这显然是在进行一次特定的旅程。传统架构难以有效地处理长序列，因为计算成本随序列长度快速增长。\n\nGEM 通过金字塔并行结构克服了这一点。可以将其想象成在底层分块处理你的行为历史，然后将这些分块组合成中间层更广泛的模式，最后在顶层将所有信息合成，形成完整的旅程理解。多个分块可以同时处理而非顺序处理，这显著提高了效率。\n\n这里的突破是规模。GEM 现在可以分析你过去数千个行为，而不仅仅是最近的几个。这种扩展视图揭示了更短窗口根本无法捕捉到的模式，例如从随意兴趣到认真购买意图的渐进发展，这可能需要数月时间。\n\n请参见下图：\n(原文配图，此处省略)\n\n第三个系统，名为 InterFormer，通过连接你的静态画像和行为时间线来处理跨特征学习。这正是 GEM 智能真正显现的地方。以前的方法会将你的整个行为历史压缩成一个紧凑的摘要向量（就像将一部完整的小说缩减为单一评分）。这种压缩不可避免地会丢失你旅程中的关键细节。\n\nInterFormer 采用一种不同的方法，使用交错结构。它在纯粹专注于理解你的行为序列的层和将这些行为与你的画像属性连接起来的层之间交替。\n\n*   第一个序列层可能识别出你对健身的兴趣随时间增长。\n*   第一个跨特征层然后考虑你的年龄、收入和位置上下文如何塑造这种健身兴趣的含义。\n*   第二个序列层用这些新洞察重新审视你的行为，可能会注意到你在工作地点附近开设健身房后，你的健身研究变得更加深入。\n*   第二个跨特征层然后对购买意图和时机做出更深层次的连接。\n\n这种交替过程通过多个层持续进行，每个周期都在不丢失完整行为记录的情况下，循环地完善理解。\n\n## 使用 GEM 的实际问题\n\n尽管 GEM 具有明显的优势，Meta 在使用 GEM 时面临一个根本性的工程挑战。\n\nGEM 规模庞大，使用数千个 GPU 经过长时间训练。直接为每次广告预测运行 GEM 将是慢得无法接受且成本高昂的。当用户滚动浏览 Instagram 时，系统需要在几十毫秒内做出广告决策。GEM 在同时服务数十亿用户时根本无法以这种速度运行。\n\nMeta 的解决方案是采用教师-学生架构，其中 GEM 充当主导师，训练数百个更小、更快的垂直模型 (VMs)，这些 VMs 实际在生产环境中提供广告服务。这些 VMs 针对特定上下文进行专门化，例如 Instagram Stories 的点击预测或 Facebook Feed 的转化预测。每个 VM 都足够轻量，可以在几毫秒内做出预测，但它们比独立训练时更智能，因为它们从 GEM 中学习。\n\n知识迁移通过两种策略进行。当 VM 在与 GEM 训练的相同领域运作，具有相似数据和目标时，直接迁移 (Direct transfer) 起作用。GEM 可以直接教授这些模型。当 VMs 在与 GEM 训练领域截然不同的专业领域工作时，分层迁移 (Hierarchical transfer) 则适用。在这些情况下，GEM 首先为 Instagram 或 Facebook Marketplace 等领域训练中型领域特定基础模型。然后，这些领域模型再训练更小的 VMs。知识通过层级向下流动，在每个阶段得到适应和专门化。\n\nMeta 采用了三种复杂技术来最大化迁移效率：\n\n*   **带有 Student Adapter 的知识蒸馏 (Knowledge distillation with Student Adapter)**：学生模型学习复制 GEM 的推理过程，而不仅仅是最终预测。Student Adapter 使用最新的真实数据优化 GEM 的预测，调整时间延迟和领域特定差异。\n*   **表示学习 (Representation learning)**：在教师和学生之间创建共享的概念框架。GEM 学习以易于在不同模型大小之间迁移的方式编码信息，在广告服务期间不增加计算开销。\n*   **参数共享 (Parameter sharing)**：这使得 VMs 能够选择性地直接从 GEM 中整合特定组件。小型 VMs 保持快速，同时借用 GEM 的复杂组件来执行复杂的用户理解任务。\n\n这三种技术共同实现了单独使用标准知识蒸馏两倍的效果。持续改进循环的工作方式如下：\n\n*   用户实时与快速的 VMs 互动。\n*   他们的互动数据流回 Meta 的数据管道。\n*   GEM 定期基于这些新数据进行再训练，更新的知识通过训练后技术迁移到 VMs。\n*   改进后的 VMs 部署到生产环境。\n\n这个循环持续重复，GEM 变得越来越智能，VMs 定期获取智能更新。\n\n## 空前规模的训练\n\n构建 GEM 要求 Meta 从头开始重建其训练基础设施。\n\n挑战在于以 LLM（大语言模型）的规模训练一个模型，但任务是推荐而非语言生成，这与 LLM 根本不同。该公司实现了有效训练吞吐量增加 23 倍，同时使用了 16 倍的 GPU，并将硬件效率提高了 1.43 倍。\n\n这需要多个领域的创新。多维并行 (Multi-dimensional parallelism) 协调数千个 GPU 如何协同工作，使用像 Hybrid Sharded Distributed Parallel 这样的技术分割模型的密集组件，并通过数据并行和模型并行的结合处理像嵌入表 (embedding tables) 这样的稀疏组件。目标是确保每个 GPU 都保持忙碌，并最大限度地减少等待来自其他 GPU 通信的空闲时间。\n\n系统级优化进一步提高了 GPU 利用率：\n\n*   **专为可变长度用户序列设计的自定义 GPU 内核 (Custom GPU kernels)**，融合操作以减少内存带宽瓶颈。\n*   **PyTorch 2.0 图级编译 (graph-level compilation)** 自动化激活检查点 (activation checkpointing) 和操作符融合 (operator fusion) 等优化。\n*   **内存压缩 (Memory compression)**，包括 FP8 量化 (quantization) 以减少占用空间而不影响准确性。\n*   **NCCLX 通信集合 (communication collectives)** 处理 GPU 间通信，不占用主要计算资源。\n\n效率提升不仅仅是原始训练速度。\n\nMeta 通过优化训练器初始化、数据读取器设置和检查点 (checkpointing)，将作业启动时间缩短了 5 倍。他们通过智能缓存策略将 PyTorch 2.0 编译时间缩短了 7 倍。这些看似微不足道的细节，但在训练消耗数百万美元计算资源的模型时，每提高一个百分点的效率都至关重要。\n\n结果是一个能够快速迭代 GEM 的训练系统，以过去基础设施无法达到的速度整合新数据和架构改进。这使 Meta 能够将 GEM 保持在推荐 AI 的前沿，同时足够控制成本，使这项巨大投资物有所值。\n\n## 结论\n\nMeta 的 GEM 路线图远远超出了其当前能力。\n\n下一个主要演进涉及真正的多模态学习 (multimodal learning)，即 GEM 同时处理文本、图像、音频和视频，而不是将它们视为单独的输入流。这将对用户偏好和广告创意效果在所有内容类型上实现更丰富的理解。该公司还在探索推理时扩展 (inference-time scaling)，这将允许系统动态地为困难的预测分配更多计算资源，同时更高效地处理直接的案例。\n\n或许最雄心勃勃的是，Meta 设想一个统一互动模型 (unified engagement model)，使用相同的底层智能对有机内容和广告进行排名。这将从根本上改变广告如何融入社交信息流，可能创造更无缝的体验，让广告感觉像是自然的内容推荐而不是中断。在广告商方面，GEM 的智能将实现更复杂的代理自动化 (agentic automation)，其中 AI 系统可以以最少的人工干预管理和优化广告活动，同时实现更好的结果。\n\n## 参考文献:\n*   Meta’s Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation\n*   InterFormer Research Paper\n*   Wukong: Towards a Scaling Law for Large-Scale Recommendation\n\n---\n\n## 要点总结\n\n*   **核心挑战与 GEM 的目标**：传统广告推荐系统难以处理跨平台、数据稀疏、动态特征和多模态输入。GEM 旨在通过统一智能，在 Meta 整个生态系统中全面理解用户，优化特定场景。\n*   **GEM 的用户理解架构**：GEM 通过三个互补系统理解用户：非序列特征系统（基于 Wukong 架构和可堆叠因子分解机）、序列特征系统（采用金字塔并行结构处理长序列）和 InterFormer（通过交错结构连接静态画像和行为时间线进行跨特征学习）。\n*   **教师-学生架构**：由于 GEM 本身计算成本过高，无法直接服务实时请求，Meta 采用教师-学生架构。GEM 作为“教师”训练数百个更小、更快的“垂直模型 (VMs)”，这些 VMs 在生产环境中提供广告服务。\n*   **知识迁移策略**：知识从 GEM 迁移到 VMs 的策略包括直接迁移和分层迁移，并结合了三种核心技术：带有 Student Adapter 的知识蒸馏、表示学习和参数共享，以最大化迁移效率。\n*   **持续改进循环**：系统通过用户互动数据反馈给 GEM 进行再训练，然后将更新的知识迁移到 VMs 并部署，形成一个不断学习和优化的闭环。\n*   **空前规模的训练基础设施**：Meta 为 GEM 的训练从头构建了基础设施，实现了 LLM 规模的推荐模型训练，大幅提升了训练吞吐量和硬件效率。\n*   **关键的训练优化技术**：高效训练依赖于多维并行（处理密集和稀疏组件）、自定义 GPU 内核、PyTorch 2.0 图级编译（包括激活检查点和操作符融合）以及内存压缩（如 FP8 量化）。\n*   **工程效率提升**：通过优化训练器初始化、数据读取器设置、检查点和智能缓存策略，Meta 将作业启动和编译时间大幅缩短，提升了整体工程迭代效率。\n*   **GEM 的未来方向**：包括真正的多模态学习（处理多种媒体类型）、推理时扩展（动态分配计算资源）以及统一互动模型（将有机内容和广告一同排名），以创造更无缝的用户体验和实现更高级的代理自动化。\n\n## 你可以从这篇文章学到什么\n\n作为一名有几年经验的后端/系统设计工程师，这篇文章为你提供了在构建和优化超大规模 AI 驱动系统方面的宝贵洞察和实践经验：\n\n1.  **处理复杂性的分层架构设计**：GEM 在理解用户方面采用了“非序列特征”、“序列特征”和“跨特征学习”三个互补系统，这种将复杂问题分解为多个专业化、协同工作的子系统的思想，在设计任何大型后端系统时都非常有用。你可以学习如何将一个宏大的功能拆解为可管理、可优化的模块。\n2.  **师生模型 (知识蒸馏) 的实用策略**：当你面对一个功能强大但计算资源消耗巨大的核心模型时（例如一个复杂的机器学习模型，或一个耗时的大型复杂计算模块），“教师-学生架构”是一种非常有效的解决方案。它可以帮助你在性能、成本和准确性之间找到平衡点，在生产环境中提供快速、低成本的服务，同时保留核心模型的“智能”。这对于优化延迟敏感的服务尤其重要。\n3.  **大规模机器学习基础设施的挑战与解决方案**：文章详细描述了 Meta 如何应对 LLM 规模的推荐模型训练挑战。即使你不是 ML 工程师，理解多维并行、GPU 内核优化、内存管理（如 FP8 量化）、以及 PyTorch 2.0 编译优化等技术，能帮助你更好地与 ML 团队协作，并为任何计算密集型服务提供基础设施支持和优化思路。\n4.  **持续改进和迭代的系统思维**：从用户互动到数据回流、模型再训练、知识迁移和部署的闭环，是现代数据驱动系统不断进化的关键。这强调了监控、数据管道、A/B 测试和快速迭代在系统设计中的核心地位，确保系统能够随着业务和用户行为的变化而演进。\n5.  **特征工程和模式识别的深度思考**：GEM 如何通过 Wukong 架构处理特征交互、通过金字塔并行结构处理长序列行为，以及 InterFormer 如何整合静态与动态特征，都提供了在设计数据模型和处理业务逻辑时，对数据维度、时间序列和多源信息整合的深刻启发。这能帮助你设计更智能的数据处理流程和更准确的业务规则。\n6.  **性能与效率优化的细节考量**：文章中提到的作业启动时间、编译时间优化，以及 GPU 利用率的提升，都提醒我们，在超大规模系统中，即使是看似微小的细节，其累积效应也可能带来巨大的成本节约和效率提升。这培养了对系统每个环节进行性能剖析和优化的习惯。\n7.  **前瞻性的技术发展趋势**：了解多模态学习、推理时资源动态分配、以及统一内容与广告排名等未来方向，有助于你对技术栈进行前瞻性规划，并保持对行业最新发展趋势的敏感度，从而在职业发展中保持竞争力。",
    "url": "https://blog.bytebytego.com/p/how-meta-built-a-new-ai-powered-ads"
  }
]